{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install labml\n",
    "pip install labml_nn\n",
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Rotary Positional Embeddings (RoPE)\n",
    "summary: >\n",
    "  Annotated implementation of RoPE from paper\n",
    "  RoFormer: Enhanced Transformer with Rotary Position Embedding\n",
    "---\n",
    "\n",
    "# Rotary Positional Embeddings (RoPE)\n",
    "\n",
    "This is an implementation of\n",
    "[Rotary Positional Embeddings (RoPE)](https://papers.labml.ai/paper/2104.09864)\n",
    "in [PyTorch](https://pytorch.org).\n",
    "\n",
    "Rotary Positional Embeddings (RoPE) encode position information of tokens\n",
    "with a rotation matrix that naturally incorporates explicit relative position\n",
    "dependency.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from labml.logger import inspect\n",
    "from labml_nn.transformers.mha import MultiHeadAttention\n",
    "\n",
    "\n",
    "class RotaryPositionalEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int, base: int = 10_000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base = base\n",
    "        self.d = d\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def _build_cache(self, x: torch.Tensor):\n",
    "        # Return if cache is already built\n",
    "        if self.cos_cached is not None and x.shape[0] <= self.cos_cached.shape[0]:\n",
    "            return\n",
    "\n",
    "        # Get sequence length\n",
    "        seq_len = x.shape[0]\n",
    "\n",
    "\n",
    "        theta = 1. / (self.base ** (torch.arange(0, self.d, 2).float() / self.d)).to(x.device)\n",
    "\n",
    "        # Create position indexes `[0, 1, ..., seq_len - 1]`\n",
    "        seq_idx = torch.arange(seq_len, device=x.device).float().to(x.device)\n",
    "\n",
    "        # Calculate the product of position index and $\\theta_i$\n",
    "        idx_theta = torch.einsum('n,d->nd', seq_idx, theta)\n",
    "\n",
    "        # Concatenate so that for row $m$ we have\n",
    "        idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)\n",
    "\n",
    "        # Cache them\n",
    "        self.cos_cached = idx_theta2.cos()[:, None, None, :]\n",
    "        self.sin_cached = idx_theta2.sin()[:, None, None, :]\n",
    "\n",
    "    def _neg_half(self, x: torch.Tensor):\n",
    "        d_2 = self.d // 2\n",
    "\n",
    "        return torch.cat([-x[:, :, :, d_2:], x[:, :, :, :d_2]], dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` is the Tensor at the head of a key or a query with shape `[seq_len, batch_size, n_heads, d]`\n",
    "        \"\"\"\n",
    "        self._build_cache(x)\n",
    "\n",
    "        # Split the features, we can choose to apply rotary embeddings only to a partial set of features.\n",
    "        x_rope, x_pass = x[..., :self.d], x[..., self.d:]\n",
    "\n",
    "        # Calculate\n",
    "        neg_half_x = self._neg_half(x_rope)\n",
    "\n",
    "        x_rope = (x_rope * self.cos_cached[:x.shape[0]]) + (neg_half_x * self.sin_cached[:x.shape[0]])\n",
    "\n",
    "        return torch.cat((x_rope, x_pass), dim=-1)\n",
    "\n",
    "\n",
    "class RotaryPEMultiHeadAttention(MultiHeadAttention):\n",
    "    \"\"\"\n",
    "    ## Multi-head attention with rotary positional embeddings\n",
    "\n",
    "    We override [multi-head attention from original transformer](../mha.html).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, heads: int, d_model: int, rope_percentage: float = 0.5, dropout_prob: float = 0.0):\n",
    "        super().__init__(heads, d_model, dropout_prob)\n",
    "\n",
    "        # Rotary positional embedding layers\n",
    "        d_rope = int(self.d_k * rope_percentage)\n",
    "        self.query_rotary_pe = RotaryPositionalEmbeddings(d_rope)\n",
    "        self.key_rotary_pe = RotaryPositionalEmbeddings(d_rope)\n",
    "\n",
    "    def get_scores(self, query: torch.Tensor, key: torch.Tensor):\n",
    "        \"\"\"\n",
    "        ### Calculate scores between queries and keys\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate dot-product with RoPE\n",
    "        return torch.einsum('ibhd,jbhd->ijbh', self.query_rotary_pe(query), self.key_rotary_pe(key))\n",
    "\n",
    "\n",
    "def _test_rotary():\n",
    "    \"\"\"\n",
    "    Testing RoPE with a simple example\n",
    "    \"\"\"\n",
    "    x = torch.tensor([[1, 2, 3, 4], [4, 5, 6, 7], [7, 8, 9, 10]], dtype=torch.float)\n",
    "    x = x[:, None, None, :]\n",
    "    inspect(x)\n",
    "\n",
    "    rotary_pe = RotaryPositionalEmbeddings(3)\n",
    "    inspect(rotary_pe(x))\n",
    "\n",
    "\n",
    "_test_rotary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References : https://nn.labml.ai/transformers/rope/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
