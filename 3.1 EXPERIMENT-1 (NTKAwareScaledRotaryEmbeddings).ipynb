{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code import modules and classes from the Hugging Face Transformers library to work with pre-trained language models. Let's break down each line step by step:\n",
    "\n",
    "1. `import transformers`: This line imports the `transformers` package. This package is the Hugging Face Transformers library, which provides pre-trained models and tools for working with natural language processing tasks.\n",
    "\n",
    "2. `from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig`: This line imports specific classes and modules from the `transformers` package. Here's what each imported item does:\n",
    "\n",
    "   - `AutoModelForCausalLM`: This class represents a pre-trained language model for causal language modeling tasks. It can generate text one token at a time, taking into account the previously generated tokens.\n",
    "   \n",
    "   - `AutoTokenizer`: This class is used for tokenizing text. It converts text into numerical tokens that can be processed by the language model. Tokenization is a crucial step in natural language processing.\n",
    "   \n",
    "   - `GenerationConfig`: This class is used to configure text generation settings. It allows you to specify various options for generating text, such as the maximum length of generated sequences, temperature for controlling randomness, etc.\n",
    "\n",
    "3. `import torch`: This line imports the PyTorch library, which is a popular deep learning framework. PyTorch is used for building and training neural networks. In this context, PyTorch is used because the Hugging Face Transformers library is built on top of PyTorch, and you'll often need to work with PyTorch tensors when using these models.\n",
    "\n",
    "In summary, these lines of code set up the environment for working with pre-trained language models using the Hugging Face Transformers library. We'll typically use these imports to load pre-trained models, tokenize text, and configure text generation settings for various natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the transcript of Meta's paper of Extending context window of Large Language Models via Position Interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTENDING CONTEXT WINDOW OF LARGE LANGUAGE MODELS VIA POSITION INTERPOLATION PAPER\n",
    "prompt = '''\n",
    "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
    "\n",
    "=== BEGIN ===\n",
    "\n",
    "2306.15595v2 [cs.CL] 28 Jun 2023\n",
    "\n",
    "arXiv\n",
    "\n",
    "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
    "GUAGE MODELS VIA POSITION INTERPOLATION\n",
    "\n",
    "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
    "Meta Platforms Inc.\n",
    "{chenshouyuan, shermanwong, cli, yuandong}@meta . com\n",
    "\n",
    "1 INTRODUCTION\n",
    "\n",
    "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
    "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
    "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
    "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
    "longer context windows are preferred. However, training an LLM from scratch with long context\n",
    "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
    "context window of an existing pre-trained LLM?\n",
    "\n",
    "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
    "text window. However, empirically, we found that models trained this way adapt to long context\n",
    "windows very slowly. After training for more than 10000 batches, the effective context window\n",
    "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
    "inefficient for extending to substantially longer context windows.\n",
    "\n",
    "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
    "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
    "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
    "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
    "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
    "\n",
    "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
    "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
    "down-scale the position indices so that the maximum position index matches the previous context\n",
    "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
    "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
    "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
    "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
    "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
    "\n",
    "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
    "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
    "\n",
    "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
    "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
    "We present experimental results for extending the context window to up to 32768 from the initial\n",
    "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
    "\n",
    "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
    "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
    "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
    "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
    "encodings.\n",
    "\n",
    "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
    "tended context window. We show that models extended by Position Interpolation enjoy\n",
    "significant perplexity gains from greatly extended context windows for text modeling, and\n",
    "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
    "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
    "competitive performances.\n",
    "\n",
    "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
    "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
    "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
    "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
    "a 2048 token limit.\n",
    "\n",
    "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
    "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
    "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
    "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
    "\n",
    "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
    "codings instead.\n",
    "\n",
    "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
    "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
    "window from 2K to 8K. Recently, open source community picks it up in Reddit post ! and Github\n",
    "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
    "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
    "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
    "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
    "extrapolated ones.\n",
    "\n",
    "2 METHOD\n",
    "\n",
    "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
    "\n",
    "Transformer models require explicit positional information to be injected, typically in the form of\n",
    "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
    "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
    "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
    "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
    "follows\n",
    "\n",
    "Using RoPE, the self-attention score\n",
    "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
    "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
    "key embeddings for computing attention scores.\n",
    "\n",
    "2.2 DIRECT EXTRAPOLATION\n",
    "\n",
    "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
    "its extrapolation performance is not great . In particular, when directly extending to larger context\n",
    "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
    "comparable to untrained models.\n",
    "\n",
    "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
    "reasonably well on longer context window, but may not have the capability to leverage information\n",
    "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
    "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
    "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
    "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
    "located at location 2900.\n",
    "\n",
    "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
    "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
    "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
    "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
    "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
    "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
    "think about Eqn. 2 as basis expansion as the following:\n",
    "\n",
    "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
    "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
    "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
    "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
    "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
    "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
    "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
    "\n",
    "much larger in regions beyond.\n",
    "\n",
    "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
    "\n",
    "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
    "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
    "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
    "Formally, we replace RoPE f by {’ defined as follows\n",
    "\n",
    "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
    "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
    "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
    "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
    "and after extension, we mitigate the effect on attention score computation due to context window\n",
    "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
    "following theorem, we show that the interpolated attention score is well-behaved:\n",
    "\n",
    "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
    "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
    "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
    "than extrapolated one.\n",
    "\n",
    "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
    "the model architecture in any way. This makes it attractive in practical applications, since most\n",
    "infrastructure and optimization for the original model can be reused after the extension.\n",
    "\n",
    "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
    "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
    "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
    "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
    "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
    "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
    "acquiring new knowledge.\n",
    "\n",
    "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
    "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
    "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
    "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
    "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
    "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
    "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
    "work.\n",
    "\n",
    "3 EXPERIMENTS\n",
    "\n",
    "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
    "size, and such extension can be done with only several hundreds of training steps. We show the\n",
    "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
    "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
    "ument summarization. We also present benchmark results of the extended models on the original\n",
    "LLaMA evaluation benchmarks.\n",
    "3.1 SETUP\n",
    "\n",
    "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
    "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
    "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
    "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
    "ways.\n",
    "\n",
    "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
    "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
    "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
    "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
    "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
    "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
    "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
    "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
    "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
    "et al., 2022).\n",
    "\n",
    "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
    "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
    "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
    "RedPajama dataset (Computer, 2023).\n",
    "\n",
    "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
    "\n",
    "We evaluate the long sequence language modeling performance of our extended models and base-\n",
    "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
    "dataset (Azerbayev et al., 2022).\n",
    "\n",
    "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
    "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
    "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
    "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
    "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
    "S = 256.\n",
    "\n",
    "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
    "From the results, we found that models extended with our method enjoy a significantly improved\n",
    "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
    "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
    "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
    "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
    "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
    "\n",
    "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
    "context windows. This indicates our models can effectively make use of the longer context windows\n",
    "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
    "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
    "indicates that our method may enable extension to even longer context windows.\n",
    "\n",
    "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
    "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
    "This indicates that models extended this way have limited capability of making use of context win-\n",
    "dows longer than their pre-trained settings.\n",
    "\n",
    "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
    "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
    "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
    "of performance within original evaluation context window is expected since Position Interpolation\n",
    "forces position encodings in original context window to reside in a much narrower region, which\n",
    "may negatively affect the language model’s performance. We present more benchmark results on\n",
    "the original context window size in Section 3.4.\n",
    "\n",
    "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
    "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
    "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
    "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
    "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
    "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
    "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
    "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
    "see the models have improved steadily and achieve a significantly better perplexity.\n",
    "\n",
    "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
    "\n",
    "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
    "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
    "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
    "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
    "the document.\n",
    "\n",
    "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
    "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
    "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
    "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
    "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
    "of the model is at least k.\n",
    "\n",
    "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
    "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
    "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
    "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
    "\n",
    "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
    "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
    "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
    "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
    "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
    "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
    "indication of an acceleration in the increase of window size.\n",
    "\n",
    "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
    "\n",
    "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
    "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
    "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
    "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
    "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
    "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
    "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
    "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
    "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
    "Section 3.2.\n",
    "\n",
    "3.5 LONG DOCUMENT SUMMARIZATION\n",
    "\n",
    "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
    "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
    "for training and 972 documents for evaluation. Each document comes with a human generated\n",
    "summary. We truncate all input documents to their first 15000 tokens.\n",
    "\n",
    "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
    "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
    "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
    "\n",
    "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
    "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
    "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
    "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
    "\n",
    "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
    "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
    "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
    "the models’ outputs vs the ground-truth summaries.\n",
    "\n",
    "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
    "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
    "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
    "result suggests our models with 16384 context window can effectively handle the long document\n",
    "summarization task.\n",
    "\n",
    "=== END OF FILE ===\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TESTING THE MODEL PERFORMANCE WITHOUT ROPE SCALING OR INTERPOLATION."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code load a pre-trained language model using the Hugging Face Transformers library. Let's break down each step in great detail:\n",
    "\n",
    "**Step 1: Defining the Model Path**\n",
    "```python\n",
    "model_path = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "```\n",
    "\n",
    "In this step, a variable named `model_path` is assigned a string value. This string represents the path or identifier of the pre-trained model you want to load. The model path, `\"EleutherAI/pythia-1.4b-deduped\"`, is a reference to a specific pre-trained model provided by the EleutherAI team.\n",
    "\n",
    "**Step 2: Loading the Tokenizer**\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "```\n",
    "\n",
    "In this step, the `AutoTokenizer.from_pretrained()` method is used to load the tokenizer associated with the pre-trained model. The `from_pretrained()` method takes the `model_path` as an argument and returns an instance of the tokenizer that corresponds to the specified model.\n",
    "\n",
    "A tokenizer is responsible for converting input text into a format that the model can understand. It breaks the text into tokens, assigns unique numerical IDs to each token, and performs other preprocessing tasks.\n",
    "\n",
    "**Step 3: Load the Pre-trained Model**\n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype = torch.bfloat16).to(\"cuda\")\n",
    "```\n",
    "\n",
    "In this step, the `AutoModelForCausalLM.from_pretrained()` method is used to load the pre-trained language model. Let's break down the arguments:\n",
    "\n",
    "- `model_path`: This argument specifies the path or identifier of the pre-trained model to load, which is the same as the one used to load the tokenizer.\n",
    "- `torch_dtype = torch.bfloat16`: This argument specifies the data type used for model parameters. In this case, it's set to `torch.bfloat16`, which is a lower-precision floating-point format that can reduce memory usage. It's useful for running large models on GPUs with limited memory.\n",
    "- `.to(\"cuda\")`: After loading the model, the `.to(\"cuda\")` method is called to move the model to the GPU. This assumes that you have a CUDA-compatible GPU available for running the model. Moving the model to the GPU allows for faster inference if a compatible GPU is available.\n",
    "\n",
    "In summary, these lines of code load a pre-trained language model, associated tokenizer, and configure the model to run on a CUDA-compatible GPU with lower-precision model parameters to save memory. This setup is useful for various natural language processing tasks, such as text generation or language understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about the model used, refer : https://huggingface.co/EleutherAI/pythia-1.4b-deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype = torch.bfloat16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt token length: 5445\n"
     ]
    }
   ],
   "source": [
    "# We can see that we are using a prompt having context length greater than the general transformers context length i.e 2048\n",
    "\n",
    "print(\"Prompt token length:\", len(tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! The provided line of code is responsible for generating text using a pre-trained language model and then printing the generated text. Let's break down each step in great detail:\n",
    "\n",
    "**Step 1: Tokenization**\n",
    "```python\n",
    "inputs = tokenizer(prompt + question, return_tensors=\"pt\").to(\"cuda\")\n",
    "```\n",
    "\n",
    "- `tokenizer`: This is an instance of the Hugging Face Transformers tokenizer, which is responsible for converting text into tokens that the model can understand.\n",
    "\n",
    "- `prompt + question`: In this step, the `prompt` and `question` strings are concatenated. The resulting text is the input to the language model. Both `prompt` and `question` should be previously defined strings that you want to use as input.\n",
    "\n",
    "- `return_tensors=\"pt\"`: The `tokenizer`'s `encode` method is used to tokenize the input text, and `return_tensors` is set to `\"pt\"`, which specifies that the output should be in PyTorch tensor format. This means that the tokens will be converted into a PyTorch tensor, which is a data structure compatible with deep learning models.\n",
    "\n",
    "- `.to(\"cuda\")`: After tokenization, the resulting tensor is moved to the CUDA-compatible GPU using `.to(\"cuda\")`. This assumes that you have a GPU available and properly configured for PyTorch.\n",
    "\n",
    "**Step 2: Text Generation**\n",
    "```python\n",
    "gen_out = model.generate(**inputs, max_new_tokens=200)\n",
    "```\n",
    "\n",
    "- `model`: This is an instance of a pre-trained language model, which can be used for various natural language processing tasks, including text generation.\n",
    "\n",
    "- `.generate(**inputs, max_new_tokens=300)`: The `generate` method is called on the `model` instance. This method generates text based on the input provided. Here's what the arguments mean:\n",
    "    - `**inputs`: The double asterisk `**` before `inputs` is used to unpack the `inputs` dictionary, which contains the tokenized input text. This dictionary includes the encoded input tensors.\n",
    "    - `max_new_tokens=200`: This argument specifies the maximum number of additional tokens to generate. In this case, it's set to 200, meaning the model will generate up to 200 more tokens.\n",
    "\n",
    "**Step 3: Decoding and Printing**\n",
    "```python\n",
    "print(tokenizer.batch_decode(gen_out)[0])\n",
    "```\n",
    "\n",
    "- `tokenizer.batch_decode(gen_out)`: The `batch_decode` method of the tokenizer is used to decode the generated tokens back into human-readable text. It takes `gen_out` as input, which contains the generated tokens.\n",
    "\n",
    "- `[0]`: The generated text is obtained as a list of strings. `[0]` is used to extract the first (and typically the only) generated text from the list.\n",
    "\n",
    "- `print(...)`: Finally, the generated text is printed to the console using the `print` function.\n",
    "\n",
    "In summary, this line of code tokenizes input text, generates text based on the input using a pre-trained language model, and then prints the generated text. It's a common pattern used for various natural language processing tasks like question-answering or text completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be asking bunch of questions to the Model about the Research Paper given as prompt, and will testing the model's performance in three scenarios.\n",
    "\n",
    "1. Without linear interpolation or RoPE Scaling \n",
    "2. With linear interpolation of Rotary Embeddings\n",
    "3. With NTK Aware Scaled Rotary Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = \"\\nQuestion : What is the paper about?\"\n",
    "question2 = \"\\nPlease give me a brief summary of this research paper in a few bullet points.\"\n",
    "question3 = \"\\nPlease write me the abstract for this paper.\"\n",
    "question4 = \"\\nHow many steps was the model fine tuned for the final results? Give a short answer.\"\n",
    "question5 = \"\\nHow big is the interpolation bound compared to the extrapolation bound? Give a short answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "Question : What is the paper about? The. The.\n",
      ". The.\n",
      ".\n",
      ".\n",
      ". The.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ". The. The. The. The.s.al. The. We. We..,e. We. We. We. We. The. The. We. The. The.s.s. The.e. The. The. The.s.s. The. and.\n",
      ".\n",
      ", and. The, and.\n",
      ", the,,s,s,\n",
      ", and., and, and, and.\n",
      ", and.\n",
      ",.,\n",
      ",,.\n",
      ",\n",
      ", and.\n",
      ",., and,,, and, and, and,s,s, and,,,\n",
      ",,, 2.\n",
      ", , , , , 2.\n",
      ".\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      ",\n",
      ",\n",
      ".\n",
      ", l,\n",
      ",\n",
      ", l\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question1, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "Please give me a brief summary of this research paper in a few bullet points.\n",
      ".\n",
      ".\n",
      ". The. The.\n",
      ".\n",
      ". The. The. The. The. The. The.t. The. We. We. We... We. We. We. We. We. The. The. We. The. The.s.s. The.d. The. The. The.s.s. The. and.\n",
      ", and.\n",
      ", and, and, and.s.e.s,s.\n",
      ", and., and, and.\n",
      ", and.\n",
      ".\n",
      ",.,,. s. The, and..,,.\n",
      ", and,,, and, and, and,s,s, y, y, e,  3,\n",
      ", ,,, , , 2.\n",
      ".\n",
      ",.\n",
      "\n",
      ",\n",
      ",\n",
      ",\n",
      ", n.\n",
      ",\n",
      ",\n",
      ", l,,,\n",
      ",,\n",
      ",\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question2, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "Please write me the abstract for this paper. The. The.\n",
      ". The.\n",
      ".\n",
      ".\n",
      ".1.\n",
      ".\n",
      ".\n",
      ". The. The. The. The. The. The. The. The. We. We. We. We. We. We. We. We... The. We.s. The. The.s.s. The. The. The. The...s.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ", and. The, and.\n",
      ",s.s,s,\n",
      ", and, 3.\n",
      ", k, and.\n",
      ", and.\n",
      ",\n",
      ", t.\n",
      ", the, t.\n",
      ",\n",
      ", and.\n",
      ",,, and, d, and,s,s, t, t, t, t, 3.\n",
      ",,,,, ,, 2.\n",
      ".\n",
      ",\n",
      "\n",
      ",\n",
      ".\n",
      ",.\n",
      ".\n",
      ".\n",
      ", l.\n",
      ", o,,\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question3, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "How many steps was the model fine tuned for the final results? Give a short answer.\n",
      ".\n",
      ". The.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ". The. The.. The. The. The. The. We. We. We. We. We. We. We. We. We. We. We. We. We. We.s.s. The. We. We. The. (,s.\n",
      ", and.\n",
      ", and.\n",
      ", and, and, and.\n",
      ",s,s,s,\n",
      ", and,,,\n",
      ",,, and,,,.\n",
      ",.\n",
      ",,.\n",
      ",,, n..,,,.\n",
      ",,, and, and, and,s,s, and, y, y,,,,,\n",
      ",,,,, , ,,\n",
      ",\n",
      ",,,,\n",
      ",\n",
      ",,\n",
      ",\n",
      ",\n",
      ", l,\n",
      ",\n",
      ", l,,,\n",
      ",\n",
      ",\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question4, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "How big is the interpolation bound compared to the extrapolation bound? Give a short answer.\n",
      ".\n",
      ". The.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ". The. The.. The. The. The. The. We. We. We. We. We. We. We. We. We. We. We. We. We. We.s.s. The. We. We. The. (,s.\n",
      ", and.\n",
      ", and.\n",
      ", and, and, and.\n",
      ",s,s,s,\n",
      ", and,,,\n",
      ",,, and,,,.\n",
      ",.\n",
      ",,.\n",
      ",,, n..,,, and, s,,, and, and, and,s,s, y, y, y, y,\n",
      ", n,,,, , , 2.\n",
      ", ,,,\n",
      ",\n",
      ",,\n",
      ",\n",
      ",\n",
      ", l,\n",
      ",\n",
      ", l,,,\n",
      ",\n",
      ",\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question5, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TESTING THE MODEL PERFORMANCE WITH ROPE SCALING AND LINEAR INTERPOLATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype = torch.bfloat16, rope_scaling = {\"type\" : \"linear\" , \"factor\" : 2}).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "Question : What is the paper about?\n",
      "\n",
      "\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "The\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "1.\n",
      "\n",
      "\n",
      "1.\n",
      "1.\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question1, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "Please give me a brief summary of this research paper in a few bullet points.\n",
      "\n",
      "\n",
      "The\n",
      "The\n",
      "The\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "1.\n",
      "\n",
      "\n",
      "1.\n",
      "\n",
      "1.\n",
      "1.\n",
      "1.\n",
      "\n",
      "\n",
      "Thesis\n",
      "1\n",
      "Thesis\n",
      "Thesis\n",
      "1\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question2, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "Please write me the abstract for this paper.\n",
      "\n",
      "\n",
      "The following\n",
      "The\n",
      "The\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "1.\n",
      "\n",
      "\n",
      "1.\n",
      "\n",
      "1.\n",
      "1.\n",
      "1\n",
      "Thesis\n",
      "1\n",
      "Thesis\n",
      "1\n",
      "Thesis\n",
      "1\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question3, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "How many steps was the model fine tuned for the final results? Give a short answer.\n",
      "\n",
      "\n",
      "The\n",
      "The\n",
      "The\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "1.\n",
      "\n",
      "\n",
      "1.\n",
      "1.\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "1\n",
      "1\n",
      "Thesis\n",
      "Thesis\n",
      "1\n",
      "1\n",
      "Thesis\n",
      "Thesis\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question4, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "How big is the interpolation bound compared to the extrapolation bound? Give a short answer.\n",
      "\n",
      "\n",
      "The\n",
      "The\n",
      "The\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "1.\n",
      "\n",
      "\n",
      "1.\n",
      "1.\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "1\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "1\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "1\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "\n",
      "\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n",
      "Thesis\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question5, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TESTING THE MODEL PERFORMANCE WITH NTK Aware Scaled Rotary Embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype = torch.bfloat16, rope_scaling = {\"type\" : \"dynamic\" , \"factor\" : 2}).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "Question : What is the paper about?\n",
      "\n",
      "3.6 CONCLUSION\n",
      "\n",
      "We have shown that Position Interpolation can extend the context window of pre-trained models to substantially longer context windows. We have\n",
      "demonstrated that the models can be trained on the original context window size of 2048 to up to 32768, and can be fine-tuned for\n",
      "up to 65B. We also show that the interpolated models can be effective for long context windows.\n",
      "\n",
      "3.5.1 EXTENDING CONTEXT WINDOW\n",
      "\n",
      "3.6 EXTENDING LLaMA\n",
      "\n",
      "3.6 EXPERIMENTAL RESULTS\n",
      "\n",
      "3.6.1 EXTENDING LLaMA\n",
      "\n",
      "3.6.1 EXPERIMENTAL RESULTS\n",
      "\n",
      "3.6.1.1 EXPERIMENTAL RESULTS\n",
      "\n",
      "3.6.1.1.1.1.1.1.1.1.1.1.1.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question1, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "Please give me a brief summary of this research paper in a few bullet points.\n",
      "\n",
      "1. We show that the proposed method can extend the context window of an existing pre-trained\n",
      "LLM to much longer context windows.\n",
      "\n",
      "2. We show that the proposed method is efficient and effective.\n",
      "\n",
      "3. We show that the proposed method can effectively extend the context window of an existing\n",
      "pre-trained LLM.\n",
      "\n",
      "4. We show that the proposed method is efficient and effective.\n",
      "\n",
      "5. We show that the proposed method can effectively make use of longer context windows.\n",
      "\n",
      "6. We show that the proposed method is efficient.\n",
      "\n",
      "7. We show that the proposed method is efficient.\n",
      "\n",
      "8. We show that the proposed method is efficient.\n",
      "\n",
      "8.1 EXPERIMENTAL RESULTS\n",
      "\n",
      "We show that the proposed method is efficient.\n",
      "\n",
      "8.2 EXPERIMENTAL RESULTS\n",
      "\n",
      "We show that the proposed method is efficient.\n",
      "\n",
      "8.2.1 EXPERIM\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question2, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "Please write me the abstract for this paper.\n",
      "\n",
      "3.6 CONCLUSION\n",
      "\n",
      "We have shown that extending the context window of pre-trained models with Position Interpolation can\n",
      "be effective for long context windows. We also show that the extension is efficient and stable.\n",
      "\n",
      "4. REFERENCES\n",
      "\n",
      "1.\n",
      "\n",
      "2.\n",
      "\n",
      "3.\n",
      "\n",
      "4.\n",
      "\n",
      "5.\n",
      "\n",
      "6.\n",
      "\n",
      "7.\n",
      "\n",
      "8.\n",
      "\n",
      "9.\n",
      "\n",
      "10.\n",
      "\n",
      "11.\n",
      "\n",
      "12.\n",
      "\n",
      "13.\n",
      "\n",
      "14.\n",
      "\n",
      "15.\n",
      "\n",
      "16.\n",
      "\n",
      "17.\n",
      "\n",
      "18.\n",
      "\n",
      "19.\n",
      "\n",
      "20.\n",
      "\n",
      "21.\n",
      "\n",
      "22.\n",
      "\n",
      "23.\n",
      "\n",
      "24.\n",
      "\n",
      "25.\n",
      "\n",
      "26.\n",
      "\n",
      "27.\n",
      "\n",
      "28.\n",
      "\n",
      "29.\n",
      "\n",
      "30.\n",
      "\n",
      "31.\n",
      "\n",
      "32.\n",
      "\n",
      "33.\n",
      "\n",
      "34.\n",
      "\n",
      "35.\n",
      "\n",
      "36.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question3, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "How many steps was the model fine tuned for the final results? Give a short answer.\n",
      "\n",
      "3.6 CONCLUSION\n",
      "\n",
      "We have shown that Position Interpolation can be used to extend the context window of pre-trained\n",
      "LLMs to very long context windows. We also show that the models can be fine-tuned with very\n",
      "little fine-tuning steps. We also show that the models can be fine-tuned with very little extra\n",
      "training cost. We also show that the models can be fine-tuned with very little extra training\n",
      "time. We also show that the models can be fine-tuned with very long context windows.\n",
      "\n",
      "We also show that the models can be fine-tuned with very little extra training time.\n",
      "\n",
      "We also show that the models can be fine-tuned with very long context windows.\n",
      "\n",
      "We have shown that the models can be fine-tuned with very little extra training time.\n",
      "\n",
      "We also show that the models can be fine-tuned with very long context windows.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question4, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
      "\n",
      "=== BEGIN ===\n",
      "\n",
      "2306.15595v2 [cs.CL] 28 Jun 2023\n",
      "\n",
      "arXiv\n",
      "\n",
      "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
      "GUAGE MODELS VIA POSITION INTERPOLATION\n",
      "\n",
      "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
      "Meta Platforms Inc.\n",
      "{chenshouyuan, shermanwong, cli, yuandong}@meta. com\n",
      "\n",
      "1 INTRODUCTION\n",
      "\n",
      "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
      "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
      "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
      "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
      "longer context windows are preferred. However, training an LLM from scratch with long context\n",
      "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
      "context window of an existing pre-trained LLM?\n",
      "\n",
      "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
      "text window. However, empirically, we found that models trained this way adapt to long context\n",
      "windows very slowly. After training for more than 10000 batches, the effective context window\n",
      "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
      "inefficient for extending to substantially longer context windows.\n",
      "\n",
      "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
      "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
      "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
      "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
      "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
      "\n",
      "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
      "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
      "down-scale the position indices so that the maximum position index matches the previous context\n",
      "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
      "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
      "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
      "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
      "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
      "\n",
      "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
      "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
      "\n",
      "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
      "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
      "We present experimental results for extending the context window to up to 32768 from the initial\n",
      "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
      "\n",
      "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
      "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
      "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
      "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
      "encodings.\n",
      "\n",
      "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
      "tended context window. We show that models extended by Position Interpolation enjoy\n",
      "significant perplexity gains from greatly extended context windows for text modeling, and\n",
      "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
      "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
      "competitive performances.\n",
      "\n",
      "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
      "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
      "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
      "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
      "a 2048 token limit.\n",
      "\n",
      "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
      "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
      "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
      "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
      "\n",
      "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
      "codings instead.\n",
      "\n",
      "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
      "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
      "window from 2K to 8K. Recently, open source community picks it up in Reddit post! and Github\n",
      "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
      "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
      "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
      "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
      "extrapolated ones.\n",
      "\n",
      "2 METHOD\n",
      "\n",
      "2.1 BACKGROUND: ROTARY POSITION EMBEDDING (ROPE)\n",
      "\n",
      "Transformer models require explicit positional information to be injected, typically in the form of\n",
      "positional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n",
      "(ROPE) (Su et al., 2021), which is the position encoding used in the LLLaMA model (Touvron et al.,\n",
      "2023). Given a position index m € [0, ¢) and an embedding vector x := [zg, 71,..., 241], Where\n",
      "d is the dimension of the attention head, RoPE defines a vector-valued complex function f{x, m) as\n",
      "follows\n",
      "\n",
      "Using RoPE, the self-attention score\n",
      "is only dependent on relative position m — 7 through trigonometric functions. Here q and k are the\n",
      "query and key vector for a specific attention head. At each layer, RoPE is applied on both query and\n",
      "key embeddings for computing attention scores.\n",
      "\n",
      "2.2 DIRECT EXTRAPOLATION\n",
      "\n",
      "While the attention score in RoPE only depends on the relative positions, which is what we want,\n",
      "its extrapolation performance is not great. In particular, when directly extending to larger context\n",
      "windows unseen in the training, the perplexity may shoot up to very high numbers (i.e., > 10%),\n",
      "comparable to untrained models.\n",
      "\n",
      "Ideally, we want to see the model trained on a context window of size L = 2048 to still work\n",
      "reasonably well on longer context window, but may not have the capability to leverage information\n",
      "that appears beyond L. For example, to answer a question located at 3000, the model trained on\n",
      "maximal window size of I = 2048 cannot leverage evidences provided at location 0, but still\n",
      "can leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\n",
      "behaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\n",
      "located at location 2900.\n",
      "\n",
      "What is the reason behind? How could this happen if the attention score a,,,—,, decays as the relative\n",
      "distance |m — n/| increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n",
      "far distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\n",
      "of (Su et al., 2021) may be too loose: while it indeed decays with respect to |m — nl, the bound\n",
      "can still be quite large (i.e., the bound can be critically depends on the magnitude of v;) and thus\n",
      "vacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ¢;(s) := #93), and\n",
      "think about Eqn. 2 as basis expansion as the following:\n",
      "\n",
      "where s is the positional span between a query and a key and h; := (ga; + igaj+1){k2j — tk2j+1)\n",
      "are complex coefficients depending on q and k (here the definition of h; is exactly the same as the\n",
      "definition of k; in Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\n",
      "in Fig. 2, a, can be small in magnitude in the range of [0, 2048], but gives huge values out of the\n",
      "region. The underlying reason is that the trigonometric family {¢;} (with sufficiently large d) is\n",
      "a universal approximator and can fit any arbitrary functions. Therefore, for a, there always exist\n",
      "coefficients {h;} (i.e. key and query) that corresponds to small function values in [0, 2048] but\n",
      "\n",
      "much larger in regions beyond.\n",
      "\n",
      "2.3 PROPOSED APPROACH: POSITION INTERPOLATION (PI)\n",
      "\n",
      "In Fig. 2, thanks to the smoothness of bases functions ¢; interpolation is much more stable and will\n",
      "not lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L,\n",
      "how about we define an attention score a{s) = a(Ls/L’) where L’ is the longer context window?\n",
      "Formally, we replace RoPE f by {’ defined as follows\n",
      "\n",
      "We call this transformation on the position encoding Position Interpolation. In this step, we reduce\n",
      "position indices from [0, L') to [0, L) to match the original range of indices before computing RoPE.\n",
      "Consequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\n",
      "reduced from I’ to L. Since we align the ranges of position indices and relative distances before\n",
      "and after extension, we mitigate the effect on attention score computation due to context window\n",
      "extensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\n",
      "following theorem, we show that the interpolated attention score is well-behaved:\n",
      "\n",
      "While there is no close form for B(s) := 4/21 |Ag41(s)|, numerically it is at least larger than d, and for many positional difference s, B(s) is much larger than d\n",
      "(check Appendix B for the plot). Therefore, the interpolation bound is at least 2 - 294.73 ~ 600 x\n",
      "smaller than the extrapolation bound, and thus the interpolated attention score is much more stable\n",
      "than extrapolated one.\n",
      "\n",
      "Notably, our method of rescaling of position indices does not introduce extra weight, or modify\n",
      "the model architecture in any way. This makes it attractive in practical applications, since most\n",
      "infrastructure and optimization for the original model can be reused after the extension.\n",
      "\n",
      "Fine-tuning. We can further fine-tune the interpolated model using the next token prediction task\n",
      "with interpolated position encodings on the extended context window size using a pre-training cor-\n",
      "pus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\n",
      "only needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\n",
      "is not sensitive to the choice of examples. The reason may be that the model is only adapting to the\n",
      "new context window during the fine-tuning phase, starting from a good initialization, as opposed to\n",
      "acquiring new knowledge.\n",
      "\n",
      "Other ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\n",
      "tion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max; ||, which is the maximal\n",
      "magnitude of query/key products. If we enforce a regularization on || during LLM training, it is\n",
      "possible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\n",
      "apply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\n",
      "lated a(s) when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\n",
      "of existing LLM pre-training techniques that leverage this regularization and will leave it for future\n",
      "work.\n",
      "\n",
      "3 EXPERIMENTS\n",
      "\n",
      "We show Position Interpolation can effectively extend context window up to 32 times of the original\n",
      "size, and such extension can be done with only several hundreds of training steps. We show the\n",
      "resulting models are strong LLMs with fully effective long context windows. We demonstrate its\n",
      "performance in a number of tasks including language modeling, passkey retrieval, and long doc-\n",
      "ument summarization. We also present benchmark results of the extended models on the original\n",
      "LLaMA evaluation benchmarks.\n",
      "3.1 SETUP\n",
      "\n",
      "Model Variants. We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\n",
      "et al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\n",
      "Position Interpoloation method. Except for rescaling the position indices for models extended with\n",
      "Position Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\n",
      "ways.\n",
      "\n",
      "Training Procedure. We fine-tune all model variants using the next token prediction objective. We\n",
      "use AdamW (Loshchilov & Hutter, 2019) with 5; = 0.9 and 2 = 0.95. We use a linear learning\n",
      "rate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\n",
      "we set the learning rate to 2 x 1075 and for 33B and 65B models we set the learning rate to 1072. We\n",
      "set the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\n",
      "size, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n",
      "128 global batch size. We note that the main need of using more GPUs is memory limitation during\n",
      "fine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n",
      "(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\n",
      "et al., 2022).\n",
      "\n",
      "If not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\n",
      "steps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\n",
      "training dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\n",
      "RedPajama dataset (Computer, 2023).\n",
      "\n",
      "3.2 LONG SEQUENCE LANGUAGE MODELING\n",
      "\n",
      "We evaluate the long sequence language modeling performance of our extended models and base-\n",
      "lines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\n",
      "dataset (Azerbayev et al., 2022).\n",
      "\n",
      "We use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\n",
      "we use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\n",
      "subsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\n",
      "and truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\n",
      "context window size by using a sliding window approach following Press et al. (2022) with stride\n",
      "S = 256.\n",
      "\n",
      "In Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\n",
      "From the results, we found that models extended with our method enjoy a significantly improved\n",
      "perplexity from longer context window sizes. By increasing the context window size from 2048 to\n",
      "16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\n",
      "both datasets, -0.27 and -0.48 reductions for extending LL.aMA 13B models, and -0.14 and -0.42\n",
      "reductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n",
      "-0.3 reductions of perplexity by extending to the 8192 context window size.\n",
      "\n",
      "In general, we observed a consistent trend of our models achieving better perplexity with longer\n",
      "context windows. This indicates our models can effectively make use of the longer context windows\n",
      "to better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n",
      "32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\n",
      "indicates that our method may enable extension to even longer context windows.\n",
      "\n",
      "In contrast, we observed that models extended via the direct fine-tuning method has shown regres-\n",
      "sion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\n",
      "This indicates that models extended this way have limited capability of making use of context win-\n",
      "dows longer than their pre-trained settings.\n",
      "\n",
      "We saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\n",
      "tended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\n",
      "from 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\n",
      "of performance within original evaluation context window is expected since Position Interpolation\n",
      "forces position encodings in original context window to reside in a much narrower region, which\n",
      "may negatively affect the language model’s performance. We present more benchmark results on\n",
      "the original context window size in Section 3.4.\n",
      "\n",
      "In Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\n",
      "LLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\n",
      "evaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\n",
      "certain language modeling capability, as indicated by < 20 perplexity for extending to 8192 context\n",
      "window (in contrast, the direct extrapolation method leads to > 10% perplexity). With fine-tuning,\n",
      "we observed that the perplexity improves quickly. At 200 steps the models surpassed the original\n",
      "model’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\n",
      "using sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\n",
      "see the models have improved steadily and achieve a significantly better perplexity.\n",
      "\n",
      "3.3 MEASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\n",
      "\n",
      "We study the effective context window size, i.e. the maximum distance of a token can effectively\n",
      "attend to during inference, of our models after extension. To measure this, we follow a synthetic\n",
      "evaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\n",
      "are asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\n",
      "the document.\n",
      "\n",
      "Given a language model, we estimate the upper and lower bounds of effective context windows as\n",
      "follows. Suppose the random passkey is k tokens away from the end of the input. When a model\n",
      "persistently fails to retrieve the correct passkey value across several independent attempts, it suggests\n",
      "that the effective context window size of the model is less than k. Conversely, if a model consistently\n",
      "succeeds in retrieving the correct passkey value, we deduce that the effective context window size\n",
      "of the model is at least k.\n",
      "\n",
      "We evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\n",
      "direct fine-tuning. For each model, we use 32 different &£ uniformly spaced in the targeted context\n",
      "window L’ and run the above tests for 10 times for each k, where each time a random passkey of 5\n",
      "random digits is used. In Table 4, we report kyax as a function of the number of fine-tuning steps,\n",
      "\n",
      "We can see that models extended via Position Interpolation all successfully attain their desired ex-\n",
      "tension objectives in terms of effective context window sizes, indicating by the effective context\n",
      "window size reaching maximum kp, = L/, after merely fine-tuning for 200 steps, consistently\n",
      "across both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLLaMA models\n",
      "that are extended via direct fine-tuning only saw a minimal increase of the effective context win-\n",
      "dow size kay from 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\n",
      "indication of an acceleration in the increase of window size.\n",
      "\n",
      "3.4 BENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\n",
      "\n",
      "We evaluate the models extended by Position Interpolation on several standard benchmark tasks\n",
      "within the original context window size of 2048. The evaluation results are listed in Table 5. From\n",
      "the results, we saw that models extended to 8192 produce comparable results on the original bench-\n",
      "mark which is designed for a much smaller context window, with a degradation of up to 2% on\n",
      "the benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\n",
      "dows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\n",
      "that the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\n",
      "performances, which may be due to the limited number of fine-tuning steps used in our method.\n",
      "The regression on benchmark tasks is consistent with our observation on perplexity regression in\n",
      "Section 3.2.\n",
      "\n",
      "3.5 LONG DOCUMENT SUMMARIZATION\n",
      "\n",
      "In this task, we evaluate our models’ performance on the long document summarization task. In\n",
      "particular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\n",
      "for training and 972 documents for evaluation. Each document comes with a human generated\n",
      "summary. We truncate all input documents to their first 15000 tokens.\n",
      "\n",
      "We fine-tune the LL.aMA models extended with Position Interpolation with a context window of\n",
      "16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n",
      "Model Size Context Window Fine-tune on  BoolQ PIQA Race-M Race-H WinoGrande\n",
      "\n",
      "format the raw document using the prompt template in Figure 4, and then concatenate the prompt\n",
      "with the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\n",
      "tune the model using the next token prediction task with the above setup for 10 epochs. The losses\n",
      "from the input prompt proportion of training examples are excluded during our fine-tuning.\n",
      "\n",
      "We use a generation temperature of 0.5 and top, = 0.95 as our inference parameter to generate a\n",
      "summarization of each document in the test set. The final output is truncated at 1000 tokens. We\n",
      "used the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\n",
      "the models’ outputs vs the ground-truth summaries.\n",
      "\n",
      "In Table 6 we report our evaluation results. We have also included results from two baselines in\n",
      "existing SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\n",
      "obtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\n",
      "result suggests our models with 16384 context window can effectively handle the long document\n",
      "summarization task.\n",
      "\n",
      "=== END OF FILE ===\n",
      "\n",
      "\n",
      "How big is the interpolation bound compared to the extrapolation bound? Give a short answer.\n",
      "\n",
      "3.6 CONCLUSION\n",
      "\n",
      "We have shown that the context window of LLaMA models can be extended to much longer context\n",
      "windows. We also show that the models can be fine-tuned for longer context windows.\n",
      "Empirically, we have shown that the models can adapt to longer context windows.\n",
      "\n",
      "We also show that the models can be fine-tuned for longer context windows.\n",
      "\n",
      "3.6 EXPERIMENTS\n",
      "\n",
      "3.6.1 EXTENDING LLaMA\n",
      "\n",
      "We have shown that LLaMA models can be fine-tuned for longer context windows.\n",
      "\n",
      "3.6.2 EXTENDING LLaMA\n",
      "\n",
      "We have shown that LLaMA models can be fine-tuned for longer context windows.\n",
      "\n",
      "3.6.2.1 EXTENDING LLaMA\n",
      "\n",
      "We have shown that LLaMA models can be fine-tuned for longer context windows.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt + question5, return_tensors= \"pt\").to(\"cuda\")\n",
    "gen_out = model.generate(**inputs , max_new_tokens = 200)\n",
    "print(tokenizer.batch_decode(gen_out)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment Summary:**\n",
    "\n",
    "In the conducted experiment, we explored the performance of different approaches for extending the context length of the \"EleutherAI/pythia-1.4b-deduped\" Large Language Model (LLM). We used a lengthy prompt extracted from Meta's Research Paper on extending context length in language models via positional interpolation. We asked the LLM a series of questions related to the prompt under three different scenarios:\n",
    "\n",
    "**Scenarios:**\n",
    "\n",
    "1. **No Positional Interpolation or Scaled Rotary Embeddings**: In this scenario, no specialized techniques were applied to extend the context length. The model's outputs were gibberish and consisted mainly of predicted tokens or characters. The LLM struggled to provide meaningful answers.\n",
    "\n",
    "2. **Linear NTK-Aware Scaled Rotary Embeddings**: Here, we introduced positional interpolation or scaled rotary embeddings to enhance the model's positional encoding. While the outputs were still not entirely coherent, the model managed to generate words like \"Thesis\" and showed some improvement compared to Scenario 1.\n",
    "\n",
    "3. **Dyanmic NTK-Aware Scaled Rotary Embeddings**: In the third scenario, we incorporated NTK-aware scaled rotary embeddings. This approach outperformed the previous scenarios, producing meaningful sentences and providing more accurate answers to the questions.\n",
    "\n",
    "**Observations:**\n",
    "\n",
    "- **Scenario 1 (No Special Techniques)**: The absence of specialized techniques for handling long-context sequences resulted in gibberish outputs. The model struggled to make sense of the lengthy prompt and generated random tokens.\n",
    "\n",
    "- **Scenario 2 (Linear NTK-Aware Scaled Rotary Embeddings)**: The introduction of positional interpolation or scaled rotary embeddings demonstrated some improvement. While the outputs were not entirely coherent, the model could generate recognizable words. However, the overall quality remained low.\n",
    "\n",
    "- **Scenario 3 (Dyanmic NTK-Aware Scaled Rotary Embeddings)**: The incorporation of NTK-aware scaled rotary embeddings led to a significant improvement in model performance. The outputs were more coherent and meaningful, and the model provided more accurate answers to the questions.\n",
    "\n",
    "**Explanation for NTK-Aware Scaled Rotary Embeddings' Superior Performance:**\n",
    "\n",
    "The superior performance of the NTK-aware scaled rotary embeddings can be attributed to their adaptability and effectiveness in handling long-context sequences:\n",
    "\n",
    "1. **Adaptability**: NTK-aware scaled rotary embeddings are designed to adapt to varying context lengths. They dynamically adjust the positional encodings based on the input sequence length. This adaptability allows the model to capture long-range dependencies effectively.\n",
    "\n",
    "2. **Effective Positional Encoding**: The NTK-aware approach ensures that the positional encodings are optimized for the specific input, making them more informative. This optimization helps the model understand the context better, resulting in more coherent outputs.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "- In scenarios where we need to work with extended context lengths beyond the default capabilities of LLMs, employing specialized techniques for positional encoding is essential.\n",
    "\n",
    "- NTK-aware scaled rotary embeddings are a promising approach for handling long-context sequences. They outperformed other techniques in this experiment by providing more meaningful and coherent outputs.\n",
    "\n",
    "- While linear/static interpolation and basic scaled rotary embeddings showed some improvement over the absence of specialized techniques, they were not as effective as the NTK-aware approach.\n",
    "\n",
    "- The choice of positional encoding technique should be tailored to the specific requirements of the task and input data. For tasks involving extended context lengths, NTK-aware scaled rotary embeddings are a valuable option.\n",
    "\n",
    "- Overall, the experiment highlights the importance of addressing context length limitations in language models and demonstrates the potential of advanced positional encoding techniques like NTK-aware scaled rotary embeddings. Scaling LLaMA and GPTNeoX to >8k input context -- you can now get it from 🤗 transformers!

The best part? You can achieve it simply by adding a flag -- tiny 1.4B pretrained models are now able to digest docs like a scientific paper! 🔥


Let me tell you its story: 🧑‍🏫

It all started with the realization that LLaMA should generalize beyond the original length (2048) but doesn't. Despite the nice mathematical properties, the rotary position embeddings (RoPE) are problematic when we go beyond the pretrained range. So... let's squeeze them! 🤏

Linear scaling was independently and simultaneously discovered by the Reddit user /u/kaiokendev and the Meta team behind https://lnkd.in/dvwVc-H6. On models using RoPE, it is the first step towards training with short sequences and effectively expanding them with minimal fine-tuning 🚀

But oh, Reddit was not sated.

The Reddit user /u/bloc97 noticed that, if we apply Neural Tangent Kernel (NTK) theory, scaling RoPE's Fourier space linearly is suboptimal. At some point, the model loses the ability to distinguish tokens close by. So let's do NTK-aware scaling! 🤓

It was quickly pointed out by Reddit user /u/emozilla that NTK-aware scaling could be further improved by making it dynamic, scaling according to the input length. NTK and linear scaling do sacrifice performance on short sequences, which could be avoided that way 🧠

How good is dynamic scaling? Have a look at the plot I've added in the comments, by /u/emozilla. Without ANY fine-tuning, the perplexity stays low even when you massively increase the context length 🤯 
 
With the input from /u/kaiokendev, and /u/bloc97, and /u/emozilla, I'm glad to announce that you can benefit from linear and dynamic NTK-aware scaling on LLaMA... and GPTNeoX!

All you need is to pass the `rope_scaling` argument at model initialization time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
