{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fb66351-24f3-4bf3-9762-2c821369ddbd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Prompt Tuning\n",
    "This lesson introduces how to apply prompt tuning to your model of choice using [Parameter-Efficient Fine-Tuning (PEFT) library developed by HuggingFace](https://huggingface.co/docs/peft/index). This PEFT library supports multiple methods to reduce the number of parameters for fine-tuning, including prompt tuning and LoRA. For a complete list of methods, refer to their [documentation](https://huggingface.co/docs/peft/main/en/index#supported-methods). Only a subset of models and tasks are supported by this PEFT library for the time being, including GPT-2, LLaMA; for pairs of models and tasks supported, refer to this [page](https://huggingface.co/docs/peft/main/en/index#supported-models).\n",
    "\n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Apply prompt tuning to your model of choice\n",
    "1. Fine-tune on your provided dataset\n",
    "1. Save and share your model to HuggingFace hub\n",
    "1. Conduct inference using the fine-tuned model\n",
    "1. Compare outputs from randomly- and text-initialized fine-tuned model vs. foundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1862c9fc-6a7c-49df-889b-7f24b74ca51e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Requirement already satisfied: peft==0.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-48930855-ea26-492a-b50c-46bf90e5be3d/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: pyyaml in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (6.0)\n",
      "Requirement already satisfied: psutil in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (5.9.0)\n",
      "Requirement already satisfied: safetensors in /local_disk0/.ephemeral_nfs/envs/pythonEnv-48930855-ea26-492a-b50c-46bf90e5be3d/lib/python3.10/site-packages (from peft==0.4.0) (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (1.21.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (1.13.1+cpu)\n",
      "Requirement already satisfied: transformers in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (4.29.2)\n",
      "Requirement already satisfied: accelerate in /databricks/python3/lib/python3.10/site-packages (from peft==0.4.0) (0.19.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging>=20.0->peft==0.4.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /databricks/python3/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (4.3.0)\n",
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (0.15.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (4.64.1)\n",
      "Requirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from transformers->peft==0.4.0) (3.6.0)\n",
      "Requirement already satisfied: fsspec in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0) (2022.7.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers->peft==0.4.0) (3.3)\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install peft==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d597e523-e6a2-47b7-9b9d-9e959c76f5d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n",
      "| enumerating serving endpoints...found 0...(0 seconds)\n",
      "| No action taken\n",
      "\n",
      "Skipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/llm-foundation-models/v01-raw\"\n",
      "\n",
      "Validating the locally installed datasets:\n",
      "| listing local files...(3 seconds)\n",
      "| validation completed...(3 seconds total)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing lab testing framework.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the \"default\" schema.\n",
      "\n",
      "Predefined paths variables:\n",
      "| DA.paths.working_dir: /dbfs/mnt/dbacademy-users/labuser4687840@vocareum.com/llm-foundation-models\n",
      "| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/labuser4687840@vocareum.com/llm-foundation-models/database.db\n",
      "| DA.paths.datasets:    /dbfs/mnt/dbacademy-datasets/llm-foundation-models/v01-raw\n",
      "\n",
      "Setup completed (6 seconds)\n",
      "\n",
      "The models developed or used in this course are for demonstration and learning purposes only.\n",
      "Models may occasionally output offensive, inaccurate, biased information, or harmful instructions.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbfdafd-cb3c-4344-9d4d-e2a43246ada8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "[Auto Classes](https://huggingface.co/docs/transformers/main/en/model_doc/auto#auto-classes) helps you automatically retrieve the relevant model and tokenizers, given the pre-trained models you are interested in using. \n",
    "\n",
    "Causal language modeling refers to the decoding process, where the model predicts the next token based on only the tokens on the left. The model cannot see the future tokens, unlike masked language models that have full access to tokens bidirectionally. A canonical example of a causal language model is GPT-2. You also hear causal language models being described as autoregresssive as well. \n",
    "\n",
    "API docs:\n",
    "* [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer)\n",
    "* [AutoModelForCausalLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForCausalLM)\n",
    "\n",
    "In this demo, we will be using `bigscience/bloomz-560m` as our **foundation** causal LM to generate text. You can read more about [`bloomz` model here](https://huggingface.co/bigscience/bloomz). It was trained on [multi-lingual dataset](https://huggingface.co/datasets/bigscience/xP3), spanning 46 languages and 13 programming langauges. The dataset covers a wide range of NLP tasks, including Q/A, title generation, text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b4e03f-dc2e-45f2-9ffc-c83cc7380613",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "971b550a-c157-4563-b7a2-c98fecd98494",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Before doing any fine-tuning, we will ask the model to generate a new phrase to the following input sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "360f05b5-57b1-4dca-aa6c-f06e1f37cffb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two things are infinite:  the number of people and the number']\n"
     ]
    }
   ],
   "source": [
    "input1 = tokenizer(\"Two things are infinite: \", return_tensors=\"pt\")\n",
    "\n",
    "foundation_outputs = foundation_model.generate(\n",
    "    input_ids=input1[\"input_ids\"], \n",
    "    attention_mask=input1[\"attention_mask\"], \n",
    "    max_new_tokens=7, \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "print(tokenizer.batch_decode(foundation_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f808b044-f8f6-4738-9394-3ea0952902fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The output is not too bad. However, the dataset BLOOMZ is pre-trained on doesn't cover anything about inspirational English quotes. Therefore, we are going to fine-tune `bloomz-560m` on [a dataset called `Abirate/english_quotes`](https://huggingface.co/datasets/Abirate/english_quotes)  containing exclusively inspirational English quotes, with the hopes of using the fine-tuned version to generate more quotes later! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58d6e59d-bc09-4bed-bc36-33c68f774a0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n",
      "  warnings.warn(\n",
      "WARNING:datasets.builder:Found cached dataset json (/dbfs/mnt/dbacademy-datasets/llm-foundation-models/v01-raw/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be7868f00784359ad20294c98a1be2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /dbfs/mnt/dbacademy-datasets/llm-foundation-models/v01-raw/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4/cache-768c685e1cb83483.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['quote', 'author', 'tags', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "train_sample = data[\"train\"].select(range(50))\n",
    "display(train_sample) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff1bcdf-aa66-4081-bba3-8b922856e1c1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Onto fine-tuning: define PEFT configurations for random initialization\n",
    "\n",
    "Recall that prompt tuning allows both random and initialization of soft prompts or also known as virtual tokens. We will compare the model outputs from both initialization methods later. For now, we will start with random initialization, where all we provide is the length of the virtual prompt. \n",
    "\n",
    "API docs:\n",
    "* [PromptTuningConfig](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig)\n",
    "* [PEFT model](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e54f4b8-e020-422e-ae34-d55cfe973a94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007324504863471229\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,\n",
    "    num_virtual_tokens=4,\n",
    "    tokenizer_name_or_path=model_name\n",
    ")\n",
    "peft_model = get_peft_model(foundation_model, peft_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "075d8ab8-16b8-494e-a9a5-6241f77804c2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "That's the beauty of PEFT! It allows us to drastically reduce the number of trainable parameters. Now, we can proceed with using [HuggingFace's `Trainer` class](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#trainer) and its [`TrainingArugments` to define our fine-tuning configurations](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). \n",
    "\n",
    "The `Trainer` class provides user-friendly abstraction to leverage PyTorch under the hood to conduct training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f4370d-7eff-49e5-8be4-96beedd442d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "output_directory = os.path.join(DA.paths.working_dir, \"peft_outputs\") # can give some other path here\n",
    "\n",
    "if not os.path.exists(DA.paths.working_dir):\n",
    "    os.mkdir(DA.paths.working_dir)\n",
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory, # Where the model predictions and checkpoints will be written\n",
    "    no_cuda=True, # This is necessary for CPU clusters. \n",
    "    auto_find_batch_size=True, # Find a suitable batch size that will fit into memory automatically \n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning\n",
    "    num_train_epochs=5 # Number of passes to go through the entire fine-tuning dataset \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c40f44c-a0d1-47a4-a9eb-49d0002819e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train\n",
    "\n",
    "We will also use `Data Collator` to help us form batches of inputs to pass in to the model for training. Go [here](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#data-collator) for documentation.\n",
    "\n",
    "Specifically, we will be using `DataCollatorforLanguageModeling` which will additionally pad the inputs to the maximum length of a batch since the inputs can have variable lengths. Refer to [API docs here](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling).\n",
    "\n",
    "Note: This cell might take ~10 mins to train. **Decrease `num_train_epochs` above to speed up the training process.** On another hand, you might notice that this cells triggers a whole new MLflow run. [MLflow](https://mlflow.org/docs/latest/index.html) is an open source tool that helps to manage end-to-end machine learning lifecycle, including experiment tracking, ML code packaging, and model deployment. You can read more about [LLM tracking here](https://mlflow.org/docs/latest/llm-tracking.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e3332e-69be-4c6a-8ce8-2e1ec81a311c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 10:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35, training_loss=3.268965148925781, metrics={'train_runtime': 663.9015, 'train_samples_per_second': 0.377, 'train_steps_per_second': 0.053, 'total_flos': 58327152033792.0, 'train_loss': 3.268965148925781, 'epoch': 5.0})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b763e12a-de0f-43d1-8045-af925cfe9bfc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "591d310b-b29d-45c8-9f10-2fe5e4fd89a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time_now = time.time()\n",
    "peft_model_path = os.path.join(output_directory, f\"peft_model_{time_now}\")\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a39a994-5e6d-4b6e-9c1f-23b2b3d37890",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Inference\n",
    "\n",
    "You can load the model from the path that you have saved to before, and ask the model to generate text based on our input before! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40111e7-8f8d-43bf-b6d7-3c1283378f61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "loaded_model = PeftModel.from_pretrained(foundation_model, \n",
    "                                         peft_model_path, \n",
    "                                         is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ae3bae-190a-460a-a0f0-9b2533d017c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two things are infinite:  time and space. Time is the']\n"
     ]
    }
   ],
   "source": [
    "loaded_model_outputs = loaded_model.generate(\n",
    "    input_ids=input1[\"input_ids\"], \n",
    "    attention_mask=input1[\"attention_mask\"], \n",
    "    max_new_tokens=7, \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "print(tokenizer.batch_decode(loaded_model_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c09f0112-d1ed-47d4-8c6e-946d83130821",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Well, it seems like our fine-tuned model is indeed getting closer to generating inspirational quotes. \n",
    "\n",
    "\n",
    "In fact, the input above is taken from the training dataset. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/english_quote_example.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e2f8d9b-440b-414d-87af-13cef9c2a868",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Text initialization\n",
    "\n",
    "Our fine-tuned, randomly initialized model did pretty well on the quote above. Let's now compare it with the text initialization method. \n",
    "\n",
    "Notice that all we are changing is the `prompt_tuning_init` setting and we are also providing a concise text prompt. \n",
    "\n",
    "API docs\n",
    "* [prompt_tuning_init_text](https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig.prompt_tuning_init_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "641a599f-d0fc-41db-9490-9561d5d9db2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,072 || all params: 559,217,664 || trainable%: 0.0005493388706691496\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "text_peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    prompt_tuning_init_text=\"Generate inspirational quotes\", # this provides a starter for the model to start searching for the best embeddings\n",
    "    num_virtual_tokens=3, # this doesn't have to match the length of the text above\n",
    "    tokenizer_name_or_path=model_name\n",
    ")\n",
    "text_peft_model = get_peft_model(foundation_model, text_peft_config)\n",
    "print(text_peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec98619e-8748-498e-afc4-eb0fe872ba7a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 10:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35, training_loss=2.946445792061942, metrics={'train_runtime': 623.6887, 'train_samples_per_second': 0.401, 'train_steps_per_second': 0.056, 'total_flos': 58327152033792.0, 'train_loss': 2.946445792061942, 'epoch': 5.0})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_trainer = Trainer(\n",
    "    model=text_peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "text_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429e50de-43f2-4398-af25-641bff713d3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two things are infinite:  the number of people you can count']\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "time_now = time.time()\n",
    "text_peft_model_path = os.path.join(output_directory, f\"text_peft_model_{time_now}\")\n",
    "text_trainer.model.save_pretrained(text_peft_model_path)\n",
    "\n",
    "# Load model \n",
    "loaded_text_model = PeftModel.from_pretrained(\n",
    "    foundation_model, \n",
    "    text_peft_model_path, \n",
    "    is_trainable=False\n",
    ")\n",
    "\n",
    "# Generate output\n",
    "text_outputs = text_peft_model.generate(\n",
    "    input_ids=input1[\"input_ids\"], \n",
    "    attention_mask=input1[\"attention_mask\"], \n",
    "    max_new_tokens=7, \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "    \n",
    "print(tokenizer.batch_decode(text_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "114dad7c-a24f-4b44-bf0f-ee0f9b38fa68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can see that text initialization doesn't necessarily perform better than random initialization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad6ecd3-3e77-4dc3-9889-486008ea8096",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Share model to HuggingFace hub (optional)\n",
    "\n",
    "If you have a model that you would like to share with the rest of the HuggingFace community, you can choose to push your model to the HuggingFace hub! \n",
    "\n",
    "1. You need to first create a free HuggingFace account! The signup process is simple. Go to the [home page](https://huggingface.co/) and click \"Sign Up\" on the top right corner.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/hf_homepage_signup.png\" width=700>\n",
    "\n",
    "2. Once you have signed up and confirmed your email address, click on your user icon on the top right and click the `Settings` button. \n",
    "\n",
    "3. Navigate to the `Access Token` tab and copy your token. \n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/hf_token_page.png\" width=500>\n",
    "\n",
    "\n",
    "\n",
    "API docs:\n",
    "* [push_to_hub](https://huggingface.co/docs/transformers/main/en/model_sharing#share-a-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab9f2580-0994-4469-8724-5c07fbb81e51",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Alternatively, you can use HuggingFace's helper login method. This login cell below will prompt you to enter your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdb2fcec-fdcb-4006-a440-6b1c5c4f7b5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0432cc4e51af4be7be5639473543b631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f991fbee-14d9-4faf-a479-6bebac2098ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8dfc498736246d8a784f62113a150bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/17.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6aee2de27c46b799972f9c6cad6d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 1 LFS files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Ayush-1722/bloom_prompt_tuning_1701169281.6695077/commit/b369cec40a999e3eeb1e49dc15b52b8cea5c8a64', commit_message='Upload model', commit_description='', oid='b369cec40a999e3eeb1e49dc15b52b8cea5c8a64', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "hf_username = \"Ayush-1722\"\n",
    "peft_model_id = f\"{hf_username}/bloom_prompt_tuning_{time_now}\"\n",
    "trainer.model.push_to_hub(peft_model_id, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50d41257-31ad-4da8-8e4c-9533d9e7de8f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inference from model in HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c19be6b-fb97-4fef-8c17-be8bf2f80814",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1330eca917664cb38d80d9890f18ad73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_config.json:   0%|          | 0.00/442 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f86e284bd27d42559848521cc12c137a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading adapter_model.bin:   0%|          | 0.00/17.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "peft_random_model = PeftModel.from_pretrained(foundation_model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72674cb8-b8fa-4904-86c9-5a22fc10e93c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two things are infinite:  time and space. Time is the']\n"
     ]
    }
   ],
   "source": [
    "online_model_outputs = peft_random_model.generate(\n",
    "    input_ids=input1[\"input_ids\"], \n",
    "    attention_mask=input1[\"attention_mask\"], \n",
    "    max_new_tokens=7, \n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "print(tokenizer.batch_decode(online_model_outputs, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LLM 02 - Prompt Tuning with PEFT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
