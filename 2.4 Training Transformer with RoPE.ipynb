{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e6baea3b8a4026a0a03eb0ea0ee25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<pre  style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\\n<span style=\"color: #C5C1Bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Rotary Positional Embeddings (RoPE) Experiment\n",
    "summary: This experiment trains a transformer model with Rotary Positional Embeddings (RoPE) on tiny Shakespeare dataset.\n",
    "---\n",
    "\n",
    "# Rotary Positional Embeddings (RoPE) Experiment\n",
    "\n",
    "This is an annotated PyTorch experiment to train a transformer model with Rotary Positional Embeddings (RoPE).\n",
    "\"\"\"\n",
    "\n",
    "from labml import experiment\n",
    "from labml.configs import option, calculate\n",
    "from labml_nn.transformers import TransformerConfigs\n",
    "from labml_nn.transformers.basic.autoregressive_experiment import AutoregressiveTransformer, Configs\n",
    "\n",
    "\n",
    "# ### Rotary PE attention\n",
    "def _rotary_pe_mha(c: TransformerConfigs):\n",
    "    from labml_nn.transformers.rope import RotaryPEMultiHeadAttention\n",
    "    return RotaryPEMultiHeadAttention(c.n_heads, c.d_model, 1.)\n",
    "\n",
    "\n",
    "# Configuration options\n",
    "calculate(TransformerConfigs.encoder_attn, 'rotary', _rotary_pe_mha)\n",
    "calculate(TransformerConfigs.decoder_attn, 'rotary', _rotary_pe_mha)\n",
    "calculate(TransformerConfigs.decoder_mem_attn, 'rotary', _rotary_pe_mha)\n",
    "\n",
    "\n",
    "@option(Configs.model, 'rotary_pe_transformer')\n",
    "def _model(c: Configs):\n",
    "    \"\"\"\n",
    "    Create an autoregressive model and initialize weights\n",
    "    \"\"\"\n",
    "    m = AutoregressiveTransformer(c.transformer.encoder,\n",
    "                                  c.transformer.src_embed,\n",
    "                                  c.transformer.generator).to(c.device)\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create experiment\n",
    "    experiment.create(name=\"rotary_pe_transformer\", writers={'screen'})\n",
    "    # Create configs\n",
    "    conf = Configs()\n",
    "    # Override configurations\n",
    "    experiment.configs(conf, {\n",
    "        # No fixed positional embeddings\n",
    "        'transformer.src_embed': 'no_pos',\n",
    "        'transformer.tgt_embed': 'no_pos',\n",
    "\n",
    "        # Encoder with RoPE\n",
    "        'transformer.encoder_attn': 'rotary',\n",
    "\n",
    "        #\n",
    "        'model': 'rotary_pe_transformer',\n",
    "\n",
    "        # Use character level tokenizer\n",
    "        'tokenizer': 'character',\n",
    "        # Prompt separator is blank\n",
    "        'prompt_separator': '',\n",
    "        # Starting prompt for sampling\n",
    "        'prompt': 'It is ',\n",
    "        # Use Tiny Shakespeare dataset\n",
    "        'text': 'tiny_shakespeare',\n",
    "\n",
    "        # Use a context size of $256$\n",
    "        'seq_len': 512,\n",
    "        # Train for 32 epochs\n",
    "        'epochs': 32,\n",
    "        # Batch size $4$\n",
    "        'batch_size': 4,\n",
    "        # Switch between training and validation for $10$ times\n",
    "        # per epoch\n",
    "        'inner_iterations': 10,\n",
    "\n",
    "        # Model size\n",
    "        'd_model': 128,\n",
    "        'transformer.ffn.d_ff': 512,\n",
    "        'transformer.n_heads': 16,\n",
    "        'transformer.dropout': 0.0,\n",
    "\n",
    "        # Use [Noam optimizer](../../optimizers/noam.html)\n",
    "        'optimizer.optimizer': 'Noam',\n",
    "        'optimizer.learning_rate': 1.,\n",
    "\n",
    "        'dataloader_shuffle_with_replacement': True\n",
    "    })\n",
    "\n",
    "    # Set models for saving and loading\n",
    "    experiment.add_pytorch_models({'model': conf.model})\n",
    "\n",
    "    # Start the experiment\n",
    "    with experiment.start():\n",
    "        # Run training\n",
    "        conf.run()\n",
    "\n",
    "\n",
    "#\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a part of a machine learning experiment setup using the LabML library, particularly for training a Transformer-based model with Rotary Positional Encodings (RoPE) on a text dataset. Below is a step-by-step explanation of the code:\n",
    "\n",
    "1. **Importing Libraries**:\n",
    "   - The code begins by importing necessary libraries and modules from LabML and labml_nn. These libraries provide tools for configuring and running machine learning experiments.\n",
    "\n",
    "2. **Configuring Transformer with RoPE**:\n",
    "   - The `_rotary_pe_mha` function is defined to create a Rotary Positional Embedding (RoPE) Multi-Head Attention module. This is used to set up attention mechanisms in the Transformer architecture with RoPE.\n",
    "\n",
    "3. **Configuration Options**:\n",
    "   - The `calculate` function from LabML is used to set specific options for the Transformer model.\n",
    "   - `TransformerConfigs.encoder_attn`, `TransformerConfigs.decoder_attn`, and `TransformerConfigs.decoder_mem_attn` are configuration options related to attention mechanisms in the encoder and decoder. These options are set to use the Rotary PE Multi-Head Attention module created earlier.\n",
    "\n",
    "4. **Model Configuration**:\n",
    "   - The `_model` function is defined to create an autoregressive Transformer model and initialize its weights. This model uses the Transformer configuration specified in the argument, including the RoPE Multi-Head Attention.\n",
    "\n",
    "5. **Main Function**:\n",
    "   - The `main` function is defined as the main entry point of the script.\n",
    "\n",
    "6. **Creating an Experiment**:\n",
    "   - `experiment.create` is used to create a LabML experiment named \"rotary_pe_transformer\" and specifies that experiment logs will be written to the screen.\n",
    "\n",
    "7. **Configuration Setup**:\n",
    "   - An instance of the `Configs` class (presumably provided by labml_nn) is created as `conf`. This class likely contains various configuration options for the Transformer model and training process.\n",
    "\n",
    "8. **Overriding Configurations**:\n",
    "   - The `experiment.configs` function is used to override specific configurations within the `conf` object.\n",
    "   - It sets options related to data preprocessing, model architecture, training hyperparameters, and more. Notable configurations include using RoPE in the encoder, specifying the model to use RoPE (`'rotary_pe_transformer'`), character-level tokenization, dataset choice, sequence length, number of epochs, batch size, and learning rate.\n",
    "\n",
    "9. **Adding Models to Experiment**:\n",
    "   - `experiment.add_pytorch_models` is used to specify the PyTorch model to save and load during training. In this case, it adds the model defined in the `conf` object.\n",
    "\n",
    "10. **Starting Experiment**:\n",
    "    - The `experiment.start()` block begins the LabML experiment.\n",
    "\n",
    "11. **Training Execution**:\n",
    "    - Inside the experiment block, `conf.run()` is executed to initiate the training process with the configured options.\n",
    "\n",
    "12. **Main Function Execution**:\n",
    "    - The script checks whether it is being executed directly (`if __name__ == '__main__':`) and, if so, calls the `main` function to start the experiment when the script is run.\n",
    "\n",
    "In summary, this code sets up a LabML experiment for training a Transformer-based model with Rotary Positional Encodings on a text dataset. It configures various aspects of the model, data, and training process and then executes the training within the LabML framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References : https://nn.labml.ai/transformers/rope/experiment.html and https://github.com/labmlai/annotated_deep_learning_paper_implementations/tree/master/labml_nn/transformers/rope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
