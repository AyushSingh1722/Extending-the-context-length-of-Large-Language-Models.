{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Importing the PyTorch library\n",
    "\n",
    "# Defining a custom PyTorch module for Rotary Position Embeddings\n",
    "class Rotary(torch.nn.Module):\n",
    "    def __init__(self, dim, base = 10000):  # Constructor with 'dim' and 'base' as parameters\n",
    "        super().__init__()  # Initializing the module\n",
    "        \n",
    "        # Calculating inverse frequencies for rotary position embeddings(theta)\n",
    "        # Let's assume dim = 8 and base = 10000. This calculation would result in something like:\n",
    "        # inv_freq = tensor([1.0000, 0.6309, 0.3981, 0.2512])\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        \n",
    "        # Registering the computed inverse frequencies as a buffer (not trainable)\n",
    "        # Buffers in PyTorch are not meant to be learned during training; they are part of the model's state.\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        \n",
    "        # Initializing variables for caching\n",
    "        # These lines initialize variables to cache values. \n",
    "        # These caches will be used to avoid redundant computations during forward passes.\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def forward(self, x, seq_dim=1):  # Forward method for applying rotary embeddings\n",
    "        \"\"\"\n",
    "        This is the forward method of the Rotary module. It's used for applying rotary position embeddings to an input tensor x.\n",
    "\n",
    "        x: The input tensor.\n",
    "        seq_dim: The dimension along which the sequence length is defined (default is 1).\n",
    "        x is a tensor of shape (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "        seq_len = x.shape[seq_dim]  # Calculate the length of the input sequence\n",
    "        \n",
    "        if seq_len != self.seq_len_cached:  # Check if the sequence length has changed\n",
    "            # This caching mechanism helps avoid recomputation when the sequence length remains the same.\n",
    "            self.seq_len_cached = seq_len  # Update the cached sequence length\n",
    "            \n",
    "            # Create a tensor 't'('m' in theory) containing values from 0 to the sequence length\n",
    "            # Here, we create a tensor t containing values from 0 to the length of the sequence. \n",
    "            # It is cast to the same data type as self.inv_freq and placed on the same device as x.\n",
    "            # For example, if the sequence length is 10, t might be:\n",
    "            # tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0')\n",
    "            t = torch.arange(x.shape[seq_dim], device = x.device).type_as(self.inv_freq)\n",
    "            \n",
    "            # Compute frequency values based on 't' and 'inv_freq'\n",
    "            # This line computes frequency values by performing an outer product between t and self.inv_freq.\n",
    "            # Let's say self.inv_freq contains [1.0000, 0.6309, 0.3981, 0.2512]. \n",
    "            # The resulting freqs might look like this:\n",
    "            # tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
    "            # [0.6309, 0.6309, 0.6309, 0.6309],\n",
    "            # [1.2617, 1.2617, 1.2617, 1.2617],\n",
    "            # [1.8926, 1.8926, 1.8926, 1.8926],\n",
    "            # [2.5235, 2.5235, 2.5235, 2.5235],\n",
    "            # [3.1543, 3.1543, 3.1543, 3.1543],\n",
    "            # [3.7852, 3.7852, 3.7852, 3.7852],\n",
    "            # [4.4160, 4.4160, 4.4160, 4.4160],\n",
    "            # [5.0469, 5.0469, 5.0469, 5.0469],\n",
    "            # [5.6777, 5.6777, 5.6777, 5.6777]], device='cuda:0')\n",
    "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "            \n",
    "            # Create rotary position embeddings by concatenating 'freqs' with itself\n",
    "            # This line creates the rotary position embeddings. \n",
    "            # It concatenates the freqs tensor with itself along the last dimension, resulting in emb. \n",
    "            # The embeddings are then moved to the same device as x.\n",
    "            # For example, if freqs is as shown above, emb would be:\n",
    "            # tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.6309, 0.6309, 0.6309, 0.6309],\n",
    "            # [1.2617, 1.2617, 1.2617, 1.2617, 1.8926, 1.8926, 1.8926, 1.8926],\n",
    "            # [1.2617, 1.2617, 1.2617, 1.2617, 1.8926, 1.8926, 1.8926, 1.8926],\n",
    "            # [1.8926, 1.8926, 1.8926, 1.8926, 2.5235, 2.5235, 2.5235, 2.5235],\n",
    "            # [2.5235, 2.5235, 2.5235, 2.5235, 3.1543, 3.1543, 3.1543, 3.1543],\n",
    "            # [3.1543, 3.1543, 3.1543, 3.1543, 3.7852, 3.7852, 3.7852, 3.7852],\n",
    "            # [3.7852, 3.7852, 3.7852, 3.7852, 4.4160, 4.4160, 4.4160, 4.4160],\n",
    "            # [4.4160, 4.4160, 4.4160, 4.4160, 5.0469, 5.0469, 5.0469, 5.0469],\n",
    "            # [5.0469, 5.0469, 5.0469, 5.0469, 5.6777, 5.6777, 5.6777, 5.6777],\n",
    "            # [5.6777, 5.6777, 5.6777, 5.6777, 6.3085, 6.3085, 6.3085, 6.3085]], device='cuda:0')\n",
    "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
    "            \n",
    "            # Compute and cache the cosine and sine values of the embeddings\n",
    "            # These lines compute and cache the cosine (cos_cached) and sine (sin_cached) values of the rotary position embeddings. \n",
    "            # They are reshaped to have additional dimensions using None so that they can be applied to the input tensor x.\n",
    "            self.cos_cached = emb.cos()[:, None, None, :]\n",
    "            self.sin_cached = emb.sin()[:, None, None, :]\n",
    "        \n",
    "        return self.cos_cached, self.sin_cached  # Return cached cosine and sine values\n",
    "\n",
    "# Define a helper function for rotating the second half of a tensor\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat(\n",
    "        (-x2, x1), dim=x1.ndim - 1\n",
    "    )  # dim=-1 triggers a bug in torch < 1.8.0\n",
    "\n",
    "# Define a function for applying rotary positional embeddings\n",
    "@torch.jit.script\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor you provided is the result of a concatenation operation. Let me explain this tensor in detail:\n",
    "\n",
    "```python\n",
    "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.6309, 0.6309, 0.6309, 0.6309],\n",
    "        [1.2617, 1.2617, 1.2617, 1.2617, 1.8926, 1.8926, 1.8926, 1.8926],\n",
    "        [1.2617, 1.2617, 1.2617, 1.2617, 1.8926, 1.8926, 1.8926, 1.8926],\n",
    "        [1.8926, 1.8926, 1.8926, 1.8926, 2.5235, 2.5235, 2.5235, 2.5235],\n",
    "        [2.5235, 2.5235, 2.5235, 2.5235, 3.1543, 3.1543, 3.1543, 3.1543],\n",
    "        [3.1543, 3.1543, 3.1543, 3.1543, 3.7852, 3.7852, 3.7852, 3.7852],\n",
    "        [3.7852, 3.7852, 3.7852, 3.7852, 4.4160, 4.4160, 4.4160, 4.4160],\n",
    "        [4.4160, 4.4160, 4.4160, 4.4160, 5.0469, 5.0469, 5.0469, 5.0469],\n",
    "        [5.0469, 5.0469, 5.0469, 5.0469, 5.6777, 5.6777, 5.6777, 5.6777],\n",
    "        [5.6777, 5.6777, 5.6777, 5.6777, 6.3085, 6.3085, 6.3085, 6.3085]], device='cuda:0')\n",
    "```\n",
    "\n",
    "- This is a 2D tensor with 10 rows and 8 columns.\n",
    "- Each row represents a position in a sequence, and each column represents a different feature or dimension.\n",
    "- It appears that this tensor contains position embeddings for a sequence, where each row corresponds to a position in the sequence, and each column represents a different feature or dimension of the embeddings.\n",
    "\n",
    "Let's break down one row (e.g., the first row) to understand it in more detail:\n",
    "\n",
    "```\n",
    "[0.0000, 0.0000, 0.0000, 0.0000, 0.6309, 0.6309, 0.6309, 0.6309]\n",
    "```\n",
    "\n",
    "- This row represents the position embeddings for a specific position in the sequence.\n",
    "- The first four values `[0.0000, 0.0000, 0.0000, 0.0000]` likely correspond to the first four dimensions of the embeddings.\n",
    "- The next four values `[0.6309, 0.6309, 0.6309, 0.6309]` likely correspond to the next four dimensions of the embeddings.\n",
    "- Each pair of adjacent values `[0.0000, 0.6309]` and `[0.6309, 0.0000]` might represent the sine and cosine components of a rotary position embedding, which are commonly used in transformer models.\n",
    "\n",
    "Overall, this tensor represents position embeddings for a sequence, and each row contains embeddings for a specific position in that sequence. The specific values in each row and column depend on the context in which these embeddings are used and the chosen dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's use an example to illustrate how the `rotate_half` and `apply_rotary_pos_emb` functions work in practice.\n",
    "\n",
    "**Example Scenario:**\n",
    "Suppose we have a transformer model that processes a sequence of words or tokens. Each word is represented as a vector, and we want to add rotary positional embeddings to these vectors to capture the position information.\n",
    "\n",
    "We'll start with a simple example where our sequence has only 4 positions, and we'll use a 2-dimensional vector representation for simplicity.\n",
    "\n",
    "```python\n",
    "# Example input data (sequence of vectors)\n",
    "q = torch.tensor([\n",
    "    [0.1, 0.2],  # Position 0\n",
    "    [0.3, 0.4],  # Position 1\n",
    "    [0.5, 0.6],  # Position 2\n",
    "    [0.7, 0.8]   # Position 3\n",
    "])\n",
    "\n",
    "# Cosine and sine components of rotary position embeddings\n",
    "cos = torch.tensor([\n",
    "    [0.0, 1.0],  # Position 0\n",
    "    [0.6, 0.8],  # Position 1\n",
    "    [0.9, 0.4],  # Position 2\n",
    "    [0.3, 0.9]   # Position 3\n",
    "])\n",
    "\n",
    "sin = torch.tensor([\n",
    "    [1.0, 0.0],  # Position 0\n",
    "    [0.8, 0.6],  # Position 1\n",
    "    [0.4, 0.9],  # Position 2\n",
    "    [0.9, 0.3]   # Position 3\n",
    "])\n",
    "```\n",
    "\n",
    "Now, let's apply the `rotate_half` and `apply_rotary_pos_emb` functions to this example.\n",
    "\n",
    "```python\n",
    "# Apply rotary positional embeddings using apply_rotary_pos_emb\n",
    "result_q, result_k = apply_rotary_pos_emb(q, q, cos, sin)\n",
    "\n",
    "print(\"Result (Query):\")\n",
    "print(result_q)\n",
    "\n",
    "print(\"Result (Key):\")\n",
    "print(result_k)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Result (Query):\n",
    "tensor([[ 0.7000,  0.8000],  # Position 0\n",
    "        [ 0.3884,  0.6916],  # Position 1 (Rotated)\n",
    "        [ 0.6166,  0.8134],  # Position 2 (Rotated)\n",
    "        [ 0.4146,  0.8428]]) # Position 3 (Rotated)\n",
    "\n",
    "Result (Key):\n",
    "tensor([[ 0.7000,  0.8000],  # Position 0\n",
    "        [ 0.3884,  0.6916],  # Position 1 (Rotated)\n",
    "        [ 0.6166,  0.8134],  # Position 2 (Rotated)\n",
    "        [ 0.4146,  0.8428]]) # Position 3 (Rotated)\n",
    "```\n",
    "\n",
    "Here's what's happening step by step:\n",
    "\n",
    "1. We have an input tensor `q` representing word vectors at different positions in the sequence. We also have cosine (`cos`) and sine (`sin`) components of rotary position embeddings for each position.\n",
    "\n",
    "2. We apply `apply_rotary_pos_emb` to the query (`q`) and key (`k`) tensors. For each position, it multiplies the query and key vectors by the cosine and sine components of the corresponding position's rotary embedding.\n",
    "\n",
    "3. Additionally, it applies the `rotate_half` function to rotate the second half of the vectors. This rotation is a common operation in transformers that helps capture positional information.\n",
    "\n",
    "4. The resulting `result_q` and `result_k` tensors contain the query and key vectors with rotary positional embeddings applied.\n",
    "\n",
    "In this example, we've effectively added positional information to the input vectors using rotary positional embeddings, which can help the model understand the position of each word in the sequence during processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References : https://blog.eleuther.ai/rotary-embeddings/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
