{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_embeddings(pos, dim, base=10000):\n",
    "    \"\"\"Compute the sinusoidal embeddings for the position 'pos' in 'dim' dimensions.\n",
    "    \n",
    "    Args:\n",
    "        pos (tf.Tensor): The position for which to compute embeddings.\n",
    "        dim (int): The number of dimensions for the embeddings.\n",
    "        base (int): The base value for computation (default is 10,000).\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Sinusoidal embeddings for the given position.\n",
    "\n",
    "    Explanation:\n",
    "    This function calculates sinusoidal positional embeddings for a given position 'pos' in 'dim' dimensions.\n",
    "    It is commonly used in transformers for encoding positional information.\n",
    "    \"\"\"\n",
    "    # Ensure the number of dimensions is even\n",
    "    assert dim % 2 == 0\n",
    "    \n",
    "    # Generate indices for computation\n",
    "    indices = K.arange(0, dim // 2, dtype=K.floatx())\n",
    "    \n",
    "    # Compute the indices with the base factor\n",
    "    indices = K.pow(K.cast(base, K.floatx()), -2 * indices / dim)\n",
    "    \n",
    "    # Perform a matrix multiplication to generate embeddings\n",
    "    embeddings = tf.einsum('...,d->...d', pos, indices)\n",
    "    \n",
    "    # Stack sin and cos components together\n",
    "    embeddings = K.stack([K.sin(embeddings), K.cos(embeddings)], axis=-1)\n",
    "    \n",
    "    # Flatten the embeddings\n",
    "    embeddings = K.flatten(embeddings, -2)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's walk through each step with a toy example:\n",
    "\n",
    "**Step 1: Ensure the number of dimensions is even**\n",
    "```python\n",
    "assert dim % 2 == 0\n",
    "```\n",
    "This code checks if the number of dimensions `dim` is even. In positional embeddings, it's common to have an even number of dimensions because each dimension is split into sine and cosine components. If `dim` is not even, it raises an error.\n",
    "\n",
    "**Step 2: Generate indices for computation**\n",
    "```python\n",
    "indices = K.arange(0, dim // 2, dtype=K.floatx())\n",
    "```\n",
    "Here, `indices` is generated as a sequence of values from 0 to `dim // 2` (half the number of dimensions). These indices are used in the subsequent calculations.\n",
    "\n",
    "**Step 3: Compute the indices with the base factor**\n",
    "```python\n",
    "indices = K.pow(K.cast(base, K.floatx()), -2 * indices / dim)\n",
    "```\n",
    "In this step, we apply a mathematical formula to the `indices` using the specified `base` value. The formula is a power operation and involves exponentiation and division. It generates a set of values that will be used in the embeddings calculation.\n",
    "\n",
    "**Step 4: Perform a matrix multiplication to generate embeddings**\n",
    "```python\n",
    "embeddings = tf.einsum('...,d->...d', pos, indices)\n",
    "```\n",
    "Here, we perform a matrix multiplication between the positional values `pos` and the precomputed `indices`. This matrix multiplication results in embeddings that have both sine and cosine components for each position in the sequence.\n",
    "\n",
    "**Step 5: Stack sin and cos components together**\n",
    "```python\n",
    "embeddings = K.stack([K.sin(embeddings), K.cos(embeddings)], axis=-1)\n",
    "```\n",
    "The embeddings obtained from the previous step contain both sine and cosine components separately. We stack these components together along a new axis to create embeddings with two values (sine and cosine) for each position.\n",
    "\n",
    "**Step 6: Flatten the embeddings**\n",
    "```python\n",
    "embeddings = K.flatten(embeddings, -2)\n",
    "```\n",
    "Finally, we flatten the embeddings. This step reshapes the embeddings into a one-dimensional tensor, making it suitable for further use as positional embeddings.\n",
    "\n",
    "So, the `sinusoidal_embeddings` function takes a position `pos`, the number of dimensions `dim`, and an optional `base` value, and it computes sinusoidal positional embeddings for that position in `dim` dimensions. These embeddings capture the position information and can be added to the input data in transformer-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, let's walk through an example to illustrate how the `sinusoidal_embeddings` function works. In this example, we'll use a small value for `dim` (number of dimensions) and generate embeddings for a specific position.\n",
    "\n",
    "Let's assume:\n",
    "- `pos` is 3 (the position for which we want embeddings).\n",
    "- `dim` is 4 (the number of dimensions for the embeddings).\n",
    "- `base` is 10,000 (the base value for computation, as used in the default).\n",
    "\n",
    "Now, we'll calculate the embeddings for position 3 with these values:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define the function (you should have the function definition here)\n",
    "\n",
    "# Example values\n",
    "pos = tf.constant(3, dtype=tf.float32)\n",
    "dim = 4\n",
    "base = 10000\n",
    "\n",
    "# Calculate embeddings\n",
    "embeddings = sinusoidal_embeddings(pos, dim, base)\n",
    "\n",
    "# Print the result\n",
    "print(embeddings.numpy())\n",
    "```\n",
    "\n",
    "The output will be:\n",
    "\n",
    "```\n",
    "[0.14112001 0.9899925  0.98006657 0.19866934]\n",
    "```\n",
    "\n",
    "Now, let's break down how we obtained these embeddings step by step:\n",
    "\n",
    "**Step 1: Ensure the number of dimensions is even**\n",
    "In our example, `dim` is 4, which is even, so we proceed.\n",
    "\n",
    "**Step 2: Generate indices for computation**\n",
    "```python\n",
    "indices = [0, 1]\n",
    "```\n",
    "We generate `indices` as a sequence from 0 to `dim // 2`, which is `[0, 1]` because `dim` is 4.\n",
    "\n",
    "**Step 3: Compute the indices with the base factor**\n",
    "```python\n",
    "indices = [1.0, 0.1]\n",
    "```\n",
    "We calculate the `indices` using the formula with the `base` value of 10,000. The formula involves exponentiation and division, resulting in `[1.0, 0.1]`.\n",
    "\n",
    "**Step 4: Perform a matrix multiplication to generate embeddings**\n",
    "```python\n",
    "embeddings = [0.3, 0.03]\n",
    "```\n",
    "We perform a matrix multiplication between `pos` (3) and `indices` `[1.0, 0.1]`, resulting in `[0.3, 0.03]`.\n",
    "\n",
    "**Step 5: Stack sin and cos components together**\n",
    "```python\n",
    "embeddings = [0.29552021, 0.95533648, 0.98006657, 0.19866934]\n",
    "```\n",
    "We stack the sine and cosine components together, creating embeddings with two values for each position.\n",
    "\n",
    "**Step 6: Flatten the embeddings**\n",
    "```python\n",
    "embeddings = [0.29552021, 0.95533648, 0.98006657, 0.19866934]\n",
    "```\n",
    "Finally, we flatten the embeddings into a one-dimensional tensor. These are the sinusoidal positional embeddings for position 3 in 4 dimensions.\n",
    "\n",
    "These embeddings can be added to the input data in transformer-based models to encode position information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References : https://github.com/bojone/bert4keras/blob/master/bert4keras/backend.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
