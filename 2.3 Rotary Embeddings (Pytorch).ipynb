{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi, log\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# Helper functions\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def broadcat(tensors, dim = -1):\n",
    "    # Broadcast tensors along a specified dimension\n",
    "    num_tensors = len(tensors)\n",
    "    shape_lens = set(list(map(lambda t: len(t.shape), tensors)))\n",
    "    assert len(shape_lens) == 1, 'tensors must all have the same number of dimensions'\n",
    "    shape_len = list(shape_lens)[0]\n",
    "\n",
    "    dim = (dim + shape_len) if dim < 0 else dim\n",
    "    dims = list(zip(*map(lambda t: list(t.shape), tensors)))\n",
    "\n",
    "    expandable_dims = [(i, val) for i, val in enumerate(dims) if i != dim]\n",
    "    assert all([*map(lambda t: len(set(t[1])) <= 2, expandable_dims)]), 'invalid dimensions for broadcastable concatenation'\n",
    "    max_dims = list(map(lambda t: (t[0], max(t[1])), expandable_dims))\n",
    "    expanded_dims = list(map(lambda t: (t[0], (t[1],) * num_tensors), max_dims))\n",
    "    expanded_dims.insert(dim, (dim, dims[dim]))\n",
    "    expandable_shapes = list(zip(*map(lambda t: t[1], expanded_dims)))\n",
    "    tensors = list(map(lambda t: t[0].expand(*t[1]), zip(tensors, expandable_shapes)))\n",
    "    return torch.cat(tensors, dim = dim)\n",
    "\n",
    "# Rotary embedding helper functions\n",
    "def rotate_half(x):\n",
    "    # Rotate the input tensor by half along the last dimension\n",
    "    x = rearrange(x, '... (d r) -> ... d r', r = 2)\n",
    "    x1, x2 = x.unbind(dim = -1)\n",
    "    x = torch.stack((-x2, x1), dim = -1)\n",
    "    return rearrange(x, '... d r -> ... (d r)')\n",
    "\n",
    "def apply_rotary_emb(freqs, t, start_index = 0, scale = 1.):\n",
    "    # Apply rotary embeddings to input tensor 't' using the specified frequencies 'freqs'\n",
    "    freqs = freqs.to(t)\n",
    "    rot_dim = freqs.shape[-1]\n",
    "    end_index = start_index + rot_dim\n",
    "    assert rot_dim <= t.shape[-1], f'feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}'\n",
    "    t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]\n",
    "    t = (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)\n",
    "    return torch.cat((t_left, t, t_right), dim = -1)\n",
    "\n",
    "# Learned rotation helpers\n",
    "def apply_learned_rotations(rotations, t, start_index = 0, freq_ranges = None):\n",
    "    if exists(freq_ranges):\n",
    "        rotations = einsum('..., f -> ... f', rotations, freq_ranges)\n",
    "        rotations = rearrange(rotations, '... r f -> ... (r f)')\n",
    "\n",
    "    rotations = repeat(rotations, '... n -> ... (n r)', r = 2)\n",
    "    return apply_rotary_emb(rotations, t, start_index = start_index)\n",
    "\n",
    "# Classes\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        custom_freqs = None,\n",
    "        freqs_for = 'lang',\n",
    "        theta = 10000,\n",
    "        max_freq = 10,\n",
    "        num_freqs = 1,\n",
    "        learned_freq = False,\n",
    "        use_xpos = False,\n",
    "        xpos_scale_base = 512,\n",
    "        interpolate_factor = 1.,\n",
    "        theta_rescale_factor = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Initialize rotary embedding parameters\n",
    "        theta *= theta_rescale_factor ** (dim / (dim - 2))\n",
    "\n",
    "        if exists(custom_freqs):\n",
    "            freqs = custom_freqs\n",
    "        elif freqs_for == 'lang':\n",
    "            freqs = 1. / (theta ** (torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n",
    "        elif freqs_for == 'pixel':\n",
    "            freqs = torch.linspace(1., max_freq / 2, dim // 2) * pi\n",
    "        elif freqs_for == 'constant':\n",
    "            freqs = torch.ones(num_freqs).float()\n",
    "        else:\n",
    "            raise ValueError(f'unknown modality {freqs_for}')\n",
    "\n",
    "        self.cache = dict()\n",
    "        self.cache_scale = dict()\n",
    "        self.freqs = nn.Parameter(freqs, requires_grad = learned_freq)\n",
    "\n",
    "        # Interpolation factors\n",
    "        assert interpolate_factor >= 1.\n",
    "        self.interpolate_factor = interpolate_factor\n",
    "\n",
    "        # X-position (xpos) support\n",
    "        self.use_xpos = use_xpos\n",
    "        if not use_xpos:\n",
    "            self.register_buffer('scale', None)\n",
    "            return\n",
    "\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "        self.scale_base = xpos_scale_base\n",
    "        self.register_buffer('scale', scale)\n",
    "\n",
    "    def get_seq_pos(self, seq_len, device, dtype, offset = 0):\n",
    "        # Get sequence positions for rotary embeddings\n",
    "        return (torch.arange(seq_len, device = device, dtype = dtype) + offset) / self.interpolate_factor\n",
    "\n",
    "    def rotate_queries_or_keys(self, t, seq_dim = -2, offset = 0):\n",
    "        # Rotate queries or keys using rotary embeddings\n",
    "        assert not self.use_xpos, 'you must use `.rotate_queries_and_keys` method instead and pass in both queries and keys, for length extrapolatable rotary embeddings'\n",
    "        device, dtype, seq_len = t.device, t.dtype, t.shape[seq_dim]\n",
    "        freqs = self.forward(lambda: self.get_seq_pos(seq_len, device = device, dtype = dtype, offset = offset), cache_key = f'freqs:{seq_len}|offset:{offset}')\n",
    "        return apply_rotary_emb(freqs, t)\n",
    "\n",
    "    def rotate_queries_and_keys(self, q, k, seq_dim = -2):\n",
    "        # Rotate both queries and keys using rotary embeddings\n",
    "        assert self.use_xpos\n",
    "        device, dtype, seq_len = q.device, q.dtype, q.shape[seq_dim]\n",
    "        seq = self.get_seq_pos(seq_len, dtype = dtype, device = device)\n",
    "        freqs = self.forward(lambda: seq, cache_key = f'freqs:{seq_len}')\n",
    "        scale = self.get_scale(lambda: seq, cache_key = f'scale:{seq_len}').to(dtype)\n",
    "        rotated_q = apply_rotary_emb(freqs, q, scale = scale)\n",
    "        rotated_k = apply_rotary_emb(freqs, k, scale = scale ** -1)\n",
    "        return rotated_q, rotated_k\n",
    "\n",
    "    def get_scale(self, t, cache_key = None):\n",
    "        # Get scale for X-position (xpos) support\n",
    "        assert self.use_xpos\n",
    "\n",
    "        if exists(cache_key) and cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        if callable(t):\n",
    "            t = t()\n",
    "\n",
    "        scale = 1.\n",
    "        if self.use_xpos:\n",
    "            power = (t - len(t) // 2) / self.scale_base\n",
    "            scale = self.scale ** rearrange(power, 'n -> n 1')\n",
    "            scale = torch.cat((scale, scale), dim = -1)\n",
    "\n",
    "        if exists(cache_key):\n",
    "            self.cache[cache_key] = scale\n",
    "\n",
    "        return scale\n",
    "\n",
    "    def forward(self, t, cache_key = None):\n",
    "        # Calculate rotary embeddings\n",
    "        if exists(cache_key) and cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        if callable(t):\n",
    "            t = t()\n",
    "\n",
    "        freqs = self.freqs\n",
    "\n",
    "        freqs = torch.einsum('..., f -> ... f', t.type(freqs.dtype), freqs)\n",
    "        freqs = repeat(freqs, '... n -> ... (n r)', r = 2)\n",
    "\n",
    "        if exists(cache_key):\n",
    "            self.cache[cache_key] = freqs\n",
    "\n",
    "        return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly, let's break down the provided code step by step with detailed explanations:\n",
    "\n",
    "**Step 1: Import Dependencies**\n",
    "```python\n",
    "from math import pi, log\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange, repeat\n",
    "```\n",
    "- The code starts by importing necessary libraries:\n",
    "  - `math.pi` and `math.log` for mathematical constants and functions.\n",
    "  - `torch` for PyTorch functionality.\n",
    "  - `nn` for PyTorch's neural network module.\n",
    "  - `einsum` from PyTorch for Einstein summation notation.\n",
    "  - `rearrange` and `repeat` from the `einops` library for tensor manipulation.\n",
    "\n",
    "**Step 2: Define Helper Functions**\n",
    "```python\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "```\n",
    "- `exists(val)` is a helper function that checks if a value `val` exists (i.e., is not `None`). It returns `True` if the value exists, `False` otherwise.\n",
    "\n",
    "```python\n",
    "def broadcat(tensors, dim = -1):\n",
    "    # Function to broadcast tensors along a specified dimension\n",
    "    # ...\n",
    "    return torch.cat(tensors, dim = dim)\n",
    "```\n",
    "- `broadcat(tensors, dim)` is a function for broadcasting tensors along a specified dimension.\n",
    "  - It first checks if all tensors have the same number of dimensions.\n",
    "  - Then, it determines the dimension along which to broadcast (`dim`).\n",
    "  - It expands the dimensions of the tensors that are not along the specified dimension to match the shape of the tensors along the specified dimension.\n",
    "  - Finally, it concatenates the tensors along the specified dimension using `torch.cat`.\n",
    "\n",
    "**Step 3: Define Rotary Embedding Helper Functions**\n",
    "```python\n",
    "def rotate_half(x):\n",
    "    # Function to rotate the input tensor by half along the last dimension\n",
    "    # ...\n",
    "    return torch.cat((x_left, x, x_right), dim = -1)\n",
    "```\n",
    "- `rotate_half(x)` is a function to rotate the input tensor `x` by half along its last dimension.\n",
    "  - It rearranges the tensor `x` to group elements in pairs.\n",
    "  - It unbinds the tensor into two parts, `x1` and `x2`.\n",
    "  - It rotates the elements by swapping and negating them.\n",
    "  - Finally, it rearranges and concatenates the rotated elements back into the original shape.\n",
    "\n",
    "```python\n",
    "def apply_rotary_emb(freqs, t, start_index = 0, scale = 1.):\n",
    "    # Function to apply rotary embeddings to an input tensor 't' using specified frequencies 'freqs'\n",
    "    # ...\n",
    "    return torch.cat((t_left, t, t_right), dim = -1)\n",
    "```\n",
    "- `apply_rotary_emb(freqs, t, start_index, scale)` is a function to apply rotary embeddings to an input tensor `t` using the specified frequencies `freqs`.\n",
    "  - It calculates the rotary embeddings using cosine and sine functions based on the provided frequencies.\n",
    "  - The embeddings are applied to the input tensor by scaling and combining with the original tensor.\n",
    "  - The result is a tensor with rotary embeddings concatenated to the input tensor.\n",
    "\n",
    "```python\n",
    "def apply_learned_rotations(rotations, t, start_index = 0, freq_ranges = None):\n",
    "    # Function to apply learned rotations to an input tensor 't'\n",
    "    # ...\n",
    "    return apply_rotary_emb(rotations, t, start_index = start_index)\n",
    "```\n",
    "- `apply_learned_rotations(rotations, t, start_index, freq_ranges)` is a function to apply learned rotations to an input tensor `t`. It internally calls `apply_rotary_emb()` with the provided rotations.\n",
    "\n",
    "**Step 4: Define Rotary Embedding Class**\n",
    "```python\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, custom_freqs = None, freqs_for = 'lang', theta = 10000, max_freq = 10, num_freqs = 1, learned_freq = False, use_xpos = False, xpos_scale_base = 512, interpolate_factor = 1., theta_rescale_factor = 1.):\n",
    "        # Constructor for RotaryEmbedding class\n",
    "        # ...\n",
    "```\n",
    "- `RotaryEmbedding` is a PyTorch module for managing rotary embeddings.\n",
    "- The constructor `__init__()` initializes various parameters and settings for rotary embeddings:\n",
    "  - `dim`: The dimension of embeddings.\n",
    "  - `custom_freqs`: Custom frequencies for rotary embeddings (optional).\n",
    "  - `freqs_for`: Type of frequencies to use ('lang', 'pixel', 'constant').\n",
    "  - `theta`: Scaling factor for frequencies.\n",
    "  - `max_freq`: Maximum frequency for pixel frequencies.\n",
    "  - `num_freqs`: Number of frequencies for constant frequencies.\n",
    "  - `learned_freq`: Whether to learn the frequencies.\n",
    "  - `use_xpos`: Whether to use X-position (xpos) embeddings.\n",
    "  - `xpos_scale_base`: Scaling factor for xpos embeddings.\n",
    "  - `interpolate_factor`: Interpolation factor for sequence positions.\n",
    "  - `theta_rescale_factor`: Rescaling factor for theta based on dimension.\n",
    "\n",
    "**Step 5: Initialize Rotary Embeddings**\n",
    "```python\n",
    "        # Initialize rotary embedding parameters\n",
    "        theta *= theta_rescale_factor ** (dim / (dim - 2))\n",
    "```\n",
    "- The code rescales `theta` based on the dimension to mitigate issues when applying rotary embeddings to longer sequences.\n",
    "\n",
    "```python\n",
    "        if exists(custom_freqs):\n",
    "            freqs = custom_freqs\n",
    "        elif freqs_for == 'lang':\n",
    "            freqs = 1. / (theta ** (torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n",
    "        elif freqs_for == 'pixel':\n",
    "            freqs = torch.linspace(1., max_freq / 2, dim // 2) * pi\n",
    "        elif freqs_for == 'constant':\n",
    "            freqs = torch.ones(num_freqs).float()\n",
    "        else:\n",
    "            raise ValueError(f'unknown modality {freqs_for}')\n",
    "```\n",
    "- The code sets the frequencies for rotary embeddings based on different modalities:\n",
    "  - `'lang'`: Based on language modeling, using decreasing frequencies.\n",
    "  - `'pixel'`: For image data, using linearly spaced frequencies.\n",
    "  - `'constant'`: Constant frequencies.\n",
    "  - Custom frequencies can be provided using `custom_freqs`.\n",
    "\n",
    "**Step 6: Initialize Cache and Parameters**\n",
    "```python\n",
    "        self.cache = dict()\n",
    "        self.cache_scale = dict()\n",
    "        self.freqs = nn.Parameter(freqs, requires_grad = learned_freq)\n",
    "```\n",
    "- The code initializes caches for storing computed values and defines a trainable parameter `freqs` for the rotary frequencies\n",
    "\n",
    " based on the selected configuration.\n",
    "\n",
    "```python\n",
    "        # interpolation factors\n",
    "        assert interpolate_factor >= 1.\n",
    "        self.interpolate_factor = interpolate_factor\n",
    "```\n",
    "- The code sets the interpolation factor for sequence positions. It ensures that `interpolate_factor` is greater than or equal to 1.\n",
    "\n",
    "```python\n",
    "        # xpos\n",
    "        self.use_xpos = use_xpos\n",
    "        if not use_xpos:\n",
    "            self.register_buffer('scale', None)\n",
    "            return\n",
    "```\n",
    "- The code sets whether X-position (xpos) embeddings will be used. If not, it initializes a buffer for scaling, but it remains `None`.\n",
    "\n",
    "```python\n",
    "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
    "        self.scale_base = xpos_scale_base\n",
    "        self.register_buffer('scale', scale)\n",
    "```\n",
    "- If `use_xpos` is `True`, the code calculates and sets the scaling factor based on the dimension and scaling parameters.\n",
    "\n",
    "**Step 7: Define Sequence Position Calculation**\n",
    "```python\n",
    "    def get_seq_pos(self, seq_len, device, dtype, offset = 0):\n",
    "        return (torch.arange(seq_len, device = device, dtype = dtype) + offset) / self.interpolate_factor\n",
    "```\n",
    "- The `get_seq_pos()` method calculates sequence positions given sequence length, device, data type, and an optional offset. It accounts for interpolation.\n",
    "\n",
    "**Step 8: Define Rotation of Queries and Keys**\n",
    "```python\n",
    "    def rotate_queries_or_keys(self, t, seq_dim = -2, offset = 0):\n",
    "        assert not self.use_xpos, 'you must use `.rotate_queries_and_keys` method instead and pass in both queries and keys, for length extrapolatable rotary embeddings'\n",
    "        device, dtype, seq_len = t.device, t.dtype, t.shape[seq_dim]\n",
    "        freqs = self.forward(lambda: self.get_seq_pos(seq_len, device = device, dtype = dtype, offset = offset), cache_key = f'freqs:{seq_len}|offset:{offset}')\n",
    "        return apply_rotary_emb(freqs, t)\n",
    "```\n",
    "- The `rotate_queries_or_keys()` method applies rotary embeddings to either queries or keys. However, it cannot be used for length-extrapolatable rotary embeddings, so it raises an assertion error if `use_xpos` is `True`.\n",
    "- It calculates the rotary embeddings using frequencies obtained from `forward()` based on the sequence positions.\n",
    "- The rotary embeddings are applied to the input tensor `t` using `apply_rotary_emb()`.\n",
    "\n",
    "**Step 9: Define Rotation of Queries and Keys with X-Position**\n",
    "```python\n",
    "    def rotate_queries_and_keys(self, q, k, seq_dim = -2):\n",
    "        assert self.use_xpos\n",
    "        device, dtype, seq_len = q.device, q.dtype, q.shape[seq_dim]\n",
    "        seq = self.get_seq_pos(seq_len, dtype = dtype, device = device)\n",
    "        freqs = self.forward(lambda: seq, cache_key = f'freqs:{seq_len}')\n",
    "        scale = self.get_scale(lambda: seq, cache_key = f'scale:{seq_len}').to(dtype)\n",
    "        rotated_q = apply_rotary_emb(freqs, q, scale = scale)\n",
    "        rotated_k = apply_rotary_emb(freqs, k, scale = scale ** -1)\n",
    "        return rotated_q, rotated_k\n",
    "```\n",
    "- The `rotate_queries_and_keys()` method is used when `use_xpos` is `True`, and it rotates both queries and keys.\n",
    "- It calculates sequence positions and frequencies.\n",
    "- The `get_scale()` method is used to obtain the scaling factor.\n",
    "- Rotary embeddings are applied to both queries (`q`) and keys (`k`) using `apply_rotary_emb()`.\n",
    "\n",
    "**Step 10: Get Scaling Factor for X-Position**\n",
    "```python\n",
    "    def get_scale(self, t, cache_key = None):\n",
    "        assert self.use_xpos\n",
    "\n",
    "        if exists(cache_key) and cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        if callable(t):\n",
    "            t = t()\n",
    "\n",
    "        scale = 1.\n",
    "        if self.use_xpos:\n",
    "            power = (t - len(t) // 2) / self.scale_base\n",
    "            scale = self.scale ** rearrange(power, 'n -> n 1')\n",
    "            scale = torch.cat((scale, scale), dim = -1)\n",
    "\n",
    "        if exists(cache_key):\n",
    "            self.cache[cache_key] = scale\n",
    "\n",
    "        return scale\n",
    "```\n",
    "- The `get_scale()` method calculates the scaling factor for X-position (xpos) embeddings.\n",
    "- It checks the cache to see if the scaling factor has already been computed.\n",
    "- If not, it calculates the scaling factor based on the sequence positions.\n",
    "- The computed scale is cached for future use.\n",
    "\n",
    "**Step 11: Calculate Rotary Embeddings**\n",
    "```python\n",
    "    def forward(self, t, cache_key = None):\n",
    "        if exists(cache_key) and cache_key in self.cache:\n",
    "            return self.cache[cache_key]\n",
    "\n",
    "        if callable(t):\n",
    "            t = t()\n",
    "\n",
    "        freqs = self.freqs\n",
    "\n",
    "        freqs = torch.einsum('..., f -> ... f', t.type(freqs.dtype), freqs)\n",
    "        freqs = repeat(freqs, '... n -> ... (n r)', r = 2)\n",
    "\n",
    "        if exists(cache_key):\n",
    "            self.cache[cache_key] = freqs\n",
    "\n",
    "        return freqs\n",
    "```\n",
    "- The `forward()` method calculates rotary embeddings based on the provided frequencies and sequence positions.\n",
    "- It checks the cache to see if the embeddings have already been computed.\n",
    "- If not, it calculates the embeddings using Einstein summation and repeats the frequencies.\n",
    "- The computed embeddings are cached\n",
    "\n",
    " for future use.\n",
    "\n",
    "This code defines a flexible and configurable rotary embedding module that can be used in various applications, including natural language processing and computer vision, to enhance positional information in data. It provides options for selecting different types of frequencies and interpolation strategies, making it adaptable to different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotary Embeddings - Pytorch\n",
    "\n",
    "A standalone library for adding <a href=\"https://arxiv.org/abs/2104.09864\">rotary embeddings</a> to transformers in Pytorch, following its success as <a href=\"https://blog.eleuther.ai/rotary-embeddings/\">relative positional encoding</a>. Specifically it will make rotating information into any axis of a tensor easy and efficient, whether they be fixed positional or learned. This library will give you state of the art results for positional embedding, at little costs.\n",
    "\n",
    "My gut also tells me there is something <a href=\"https://www.nature.com/articles/s41593-021-00821-9\">more</a> to rotations that can be exploited in artificial neural networks.\n",
    "\n",
    "## Install\n",
    "\n",
    "```bash\n",
    "$ pip install rotary-embedding-torch\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "# instantiate the positional embedding in your transformer and pass to all your attention layers\n",
    "\n",
    "rotary_emb = RotaryEmbedding(dim = 32)\n",
    "\n",
    "# mock queries and keys - dimensions should end with (seq_len, feature dimension), and any number of preceding dimensions (batch, heads, etc)\n",
    "\n",
    "q = torch.randn(1, 8, 1024, 64) # queries - (batch, heads, seq len, dimension of head)\n",
    "k = torch.randn(1, 8, 1024, 64) # keys\n",
    "\n",
    "# apply the rotations to your queries and keys after the heads have been split out, but prior to the dot product and subsequent softmax (attention)\n",
    "\n",
    "q = rotary_emb.rotate_queries_or_keys(q)\n",
    "k = rotary_emb.rotate_queries_or_keys(k)\n",
    "\n",
    "# then do your attention with your queries (q) and keys (k) as usual\n",
    "```\n",
    "\n",
    "If you do all the steps above correctly, you should see a dramatic improvement during training\n",
    "\n",
    "## Axial Rotary Embeddings\n",
    "\n",
    "For easy use of 2d axial relative positional embedding, ie. vision transformers\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from rotary_embedding_torch import apply_rotary_emb, RotaryEmbedding, broadcat\n",
    "\n",
    "pos_emb = RotaryEmbedding(\n",
    "    dim = 32,\n",
    "    freqs_for = 'pixel',\n",
    "    max_freq = 256\n",
    ")\n",
    "\n",
    "# queries and keys for frequencies to be rotated into\n",
    "\n",
    "q = torch.randn(1, 256, 256, 64)\n",
    "k = torch.randn(1, 256, 256, 64)\n",
    "\n",
    "# get frequencies for each axial\n",
    "# -1 to 1 has been shown to be a good choice for images and audio\n",
    "\n",
    "freqs_h = pos_emb(torch.linspace(-1, 1, steps = 256), cache_key = 256)\n",
    "freqs_w = pos_emb(torch.linspace(-1, 1, steps = 256), cache_key = 256)\n",
    "\n",
    "# concat the frequencies along each axial\n",
    "# broadcat function makes this easy without a bunch of expands\n",
    "\n",
    "freqs = broadcat((freqs_h[:, None, :], freqs_w[None, :, :]), dim = -1)\n",
    "\n",
    "# rotate in frequencies\n",
    "\n",
    "q = apply_rotary_emb(freqs, q)\n",
    "k = apply_rotary_emb(freqs, k)\n",
    "```\n",
    "\n",
    "## Length Extrapolatable Rotary Embeddings\n",
    "\n",
    "In <a href=\"https://arxiv.org/abs/2212.10554v1\">this paper</a>, they were able to fix length extrapolation issue with rotary embeddings by giving it a decay similar to ALiBi. They named this technique XPos, and you can use it by setting `use_xpos = True` on initialization.\n",
    "\n",
    "This can only be used for autoregressive transformers\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "# instantiate the positional embedding in your transformer and pass to all your attention layers\n",
    "\n",
    "rotary_emb = RotaryEmbedding(\n",
    "    dim = 32,\n",
    "    use_xpos = True   # set this to True to make rotary embeddings extrapolate better to sequence lengths greater than the one used at training time\n",
    ")\n",
    "\n",
    "# mock queries and keys - dimensions should end with (seq_len, feature dimension), and any number of preceding dimensions (batch, heads, etc)\n",
    "\n",
    "q = torch.randn(1, 8, 1024, 64) # queries - (batch, heads, seq len, dimension of head)\n",
    "k = torch.randn(1, 8, 1024, 64) # keys\n",
    "\n",
    "# apply the rotations to your queries and keys after the heads have been split out, but prior to the dot product and subsequent softmax (attention)\n",
    "\n",
    "# instead of using `rotate_queries_or_keys`, you will use `rotate_queries_and_keys`, the rest is taken care of\n",
    "\n",
    "q, k = rotary_emb.rotate_queries_and_keys(q, k)\n",
    "```\n",
    "\n",
    "## Interpolating Sequence Positions\n",
    "\n",
    "This MetaAI <a href=\"https://arxiv.org/abs//2306.15595\">paper</a> proposes simply fine-tuning on interpolations of the sequence positions for extending to longer context length for pretrained models. They show this performs much better than simply fine-tuning on the same sequence positions but extended further.\n",
    "\n",
    "You can use this by setting the `interpolate_factor` on initialization to a value greater than `1.` (ex. if pretrained model was trained on 2048, setting `interpolate_factor = 2.` would allow fine-tuning to `2048 x 2. = 4096`)\n",
    "\n",
    "Update: someone in the community has reported that it does not work well. please email me if you see either a positive or negative result\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "rotary_emb = RotaryEmbedding(\n",
    "    dim = 32,\n",
    "    interpolate_factor = 2.    # add this line of code to pretrained model and fine-tune for ~1000 steps, as shown in paper\n",
    ")\n",
    "```\n",
    "\n",
    "## Citations\n",
    "\n",
    "```bibtex\n",
    "@misc{su2021roformer,\n",
    "    title   = {RoFormer: Enhanced Transformer with Rotary Position Embedding}, \n",
    "    author  = {Jianlin Su and Yu Lu and Shengfeng Pan and Bo Wen and Yunfeng Liu},\n",
    "    year    = {2021},\n",
    "    eprint  = {2104.09864},\n",
    "    archivePrefix = {arXiv},\n",
    "    primaryClass = {cs.CL}\n",
    "}\n",
    "```\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{Sun2022ALT,\n",
    "    title     = {A Length-Extrapolatable Transformer},\n",
    "    author    = {Yutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei},\n",
    "    year      = {2022}\n",
    "}\n",
    "```\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{Chen2023ExtendingCW,\n",
    "    title   = {Extending Context Window of Large Language Models via Positional Interpolation},\n",
    "    author  = {Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},\n",
    "    year    = {2023}\n",
    "}\n",
    "```\n",
    "\n",
    "```bibtex\n",
    "@misc{bloc97-2023\n",
    "    title   = {NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.}\n",
    "    author  = {/u/bloc97},\n",
    "    url     = {https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References : https://github.com/lucidrains/rotary-embedding-torch and https://github.com/lucidrains/rotary-embedding-torch/commit/947f26fb74d1b61f5c1da169e80f14cab4e94f00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
